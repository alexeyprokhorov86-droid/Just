"""
RAG Agent –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–æ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π –∏ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç—É.
–í–∫–ª—é—á–∞–µ—Ç SQL-–ø–æ–∏—Å–∫ –∏ –≤–µ–∫—Ç–æ—Ä–Ω—ã–π (—Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π) –ø–æ–∏—Å–∫ —Å —É—á—ë—Ç–æ–º —Å–≤–µ–∂–µ—Å—Ç–∏.
"""

import os
import pathlib
from dotenv import load_dotenv

env_path = pathlib.Path(__file__).parent / '.env'
load_dotenv(dotenv_path=env_path if env_path.exists() else None)

import json
import logging
import requests
import psycopg2
from psycopg2 import sql
import re
from datetime import datetime, timedelta

logger = logging.getLogger(__name__)

DB_HOST = os.getenv("DB_HOST", "localhost")
DB_PORT = os.getenv("DB_PORT", "5432")
DB_NAME = os.getenv("DB_NAME", "knowledge_base")
DB_USER = os.getenv("DB_USER", "knowledge")
DB_PASSWORD = os.getenv("DB_PASSWORD")
ROUTERAI_API_KEY = os.getenv("ROUTERAI_API_KEY")
ROUTERAI_BASE_URL = os.getenv("ROUTERAI_BASE_URL", "https://routerai.ru/api/v1")

# –ò–º–ø–æ—Ä—Ç –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
try:
    from embedding_service import vector_search, vector_search_weighted, index_telegram_message
    VECTOR_SEARCH_ENABLED = True
    logger.info("–í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ –≤–∫–ª—é—á–µ–Ω")
except ImportError:
    VECTOR_SEARCH_ENABLED = False
    logger.warning("embedding_service –Ω–µ –Ω–∞–π–¥–µ–Ω, –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ –æ—Ç–∫–ª—é—á–µ–Ω")


def get_db_connection():
    return psycopg2.connect(host=DB_HOST, port=DB_PORT, dbname=DB_NAME, user=DB_USER, password=DB_PASSWORD)


def clean_keywords(query: str) -> list:
    """–û—á–∏—â–∞–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –æ—Ç –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏."""
    clean_query = re.sub(r'[,.:;!?()"\']', ' ', query)
    keywords = [w.strip() for w in clean_query.split() if len(w.strip()) > 2]
    return keywords if keywords else [query]

def extract_time_context(question: str) -> dict:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç –≤—Ä–µ–º–µ–Ω–Ω–æ–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –∑–∞–ø—Ä–æ—Å–∞.
    
    –ï—Å–ª–∏ –≤ –∑–∞–ø—Ä–æ—Å–µ —É–∫–∞–∑–∞–Ω –ø–µ—Ä–∏–æ–¥ (–∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–π –º–µ—Å—è—Ü, –≤—á–µ—Ä–∞, –≤ —è–Ω–≤–∞—Ä–µ) ‚Äî
    –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ–∏—Å–∫–∞ –ø–æ–¥ —ç—Ç–æ—Ç –ø–µ—Ä–∏–æ–¥.
    
    –ï—Å–ª–∏ –ø–µ—Ä–∏–æ–¥ –Ω–µ —É–∫–∞–∑–∞–Ω ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ—Ç decay_days=90 –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é.
    """
    question_lower = question.lower()
    now = datetime.now()
    
    result = {
        "has_time_filter": False,
        "date_from": None,
        "date_to": None,
        "decay_days": 90,  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é 90 –¥–Ω–µ–π
        "freshness_weight": 0.25  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é
    }
    
    # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è "–∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–π/–ø–æ—Å–ª–µ–¥–Ω–∏–µ N –¥–Ω–µ–π/–Ω–µ–¥–µ–ª—å/–º–µ—Å—è—Ü–µ–≤"
    patterns = [
        # "–∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–π –º–µ—Å—è—Ü", "–∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 2 –º–µ—Å—è—Ü–∞"
        (r'–∑–∞ –ø–æ—Å–ª–µ–¥–Ω(?:–∏–π|–∏–µ|—é—é|–µ–µ)?\s*(\d+)?\s*–º–µ—Å—è—Ü', lambda m: int(m.group(1) or 1) * 30),
        (r'–∑–∞ (\d+)\s*–º–µ—Å—è—Ü', lambda m: int(m.group(1)) * 30),
        
        # "–∑–∞ –ø–æ—Å–ª–µ–¥–Ω—é—é –Ω–µ–¥–µ–ª—é", "–∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 2 –Ω–µ–¥–µ–ª–∏"  
        (r'–∑–∞ –ø–æ—Å–ª–µ–¥–Ω(?:–∏–π|–∏–µ|—é—é|–µ–µ)?\s*(\d+)?\s*–Ω–µ–¥–µ–ª', lambda m: int(m.group(1) or 1) * 7),
        (r'–∑–∞ (\d+)\s*–Ω–µ–¥–µ–ª', lambda m: int(m.group(1)) * 7),
        
        # "–∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–π –¥–µ–Ω—å", "–∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 3 –¥–Ω—è"
        (r'–∑–∞ –ø–æ—Å–ª–µ–¥–Ω(?:–∏–π|–∏–µ|—é—é|–µ–µ)?\s*(\d+)?\s*(?:–¥–µ–Ω—å|–¥–Ω—è|–¥–Ω–µ–π)', lambda m: int(m.group(1) or 1)),
        (r'–∑–∞ (\d+)\s*(?:–¥–µ–Ω—å|–¥–Ω—è|–¥–Ω–µ–π)', lambda m: int(m.group(1))),
        
        # "–∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–π –≥–æ–¥"
        (r'–∑–∞ –ø–æ—Å–ª–µ–¥–Ω(?:–∏–π|–∏–µ|—é—é|–µ–µ)?\s*–≥–æ–¥', lambda m: 365),
        (r'–∑–∞ –≥–æ–¥', lambda m: 365),
        
        # "–∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–π –∫–≤–∞—Ä—Ç–∞–ª"
        (r'–∑–∞ –ø–æ—Å–ª–µ–¥–Ω(?:–∏–π|–∏–µ|—é—é|–µ–µ)?\s*–∫–≤–∞—Ä—Ç–∞–ª', lambda m: 90),
        (r'–∑–∞ –∫–≤–∞—Ä—Ç–∞–ª', lambda m: 90),
        
        # "–≤—á–µ—Ä–∞"
        (r'\b–≤—á–µ—Ä–∞\b', lambda m: 2),
        
        # "—Å–µ–≥–æ–¥–Ω—è"
        (r'\b—Å–µ–≥–æ–¥–Ω—è\b', lambda m: 1),
        
        # "–Ω–∞ —ç—Ç–æ–π –Ω–µ–¥–µ–ª–µ"
        (r'–Ω–∞ —ç—Ç–æ–π –Ω–µ–¥–µ–ª–µ', lambda m: 7),
        (r'–Ω–∞ –ø—Ä–æ—à–ª–æ–π –Ω–µ–¥–µ–ª–µ', lambda m: 14),
        
        # "–≤ —ç—Ç–æ–º –º–µ—Å—è—Ü–µ"
        (r'–≤ —ç—Ç–æ–º –º–µ—Å—è—Ü–µ', lambda m: now.day),
        (r'–≤ –ø—Ä–æ—à–ª–æ–º –º–µ—Å—è—Ü–µ', lambda m: 60),
        
        # "–Ω–µ–¥–∞–≤–Ω–æ" - –∏—Å–ø–æ–ª—å–∑—É–µ–º 14 –¥–Ω–µ–π
        (r'\b–Ω–µ–¥–∞–≤–Ω–æ\b', lambda m: 14),
        
        # "–≤ –ø–æ—Å–ª–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è" - 30 –¥–Ω–µ–π
        (r'–≤ –ø–æ—Å–ª–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è', lambda m: 30),
    ]
    
    for pattern, days_func in patterns:
        match = re.search(pattern, question_lower)
        if match:
            result["has_time_filter"] = True
            result["decay_days"] = days_func(match)
            result["date_from"] = now - timedelta(days=result["decay_days"])
            result["date_to"] = now
            # –ï—Å–ª–∏ —É–∫–∞–∑–∞–Ω –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –ø–µ—Ä–∏–æ–¥ ‚Äî —É–≤–µ–ª–∏—á–∏–≤–∞–µ–º –≤–µ—Å —Å–≤–µ–∂–µ—Å—Ç–∏
            result["freshness_weight"] = 0.4
            break
    
    # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –º–µ—Å—è—Ü–µ–≤: "–≤ —è–Ω–≤–∞—Ä–µ", "–≤ —è–Ω–≤–∞—Ä–µ 2025"
    months = {
        '—è–Ω–≤–∞—Ä': 1, '—Ñ–µ–≤—Ä–∞–ª': 2, '–º–∞—Ä—Ç': 3, '–∞–ø—Ä–µ–ª': 4,
        '–º–∞–µ': 5, '–º–∞—è': 5, '–º–∞–π': 5, '–∏—é–Ω': 6, '–∏—é–ª': 7, '–∞–≤–≥—É—Å—Ç': 8,
        '—Å–µ–Ω—Ç—è–±—Ä': 9, '–æ–∫—Ç—è–±—Ä': 10, '–Ω–æ—è–±—Ä': 11, '–¥–µ–∫–∞–±—Ä': 12
    }
    
    if not result["has_time_filter"]:
        for month_pattern, month_num in months.items():
            match = re.search(rf'–≤\s+{month_pattern}\w*\s*(\d{{4}})?', question_lower)
            if match:
                year = int(match.group(1)) if match.group(1) else now.year
                # –ï—Å–ª–∏ –º–µ—Å—è—Ü –≤ –±—É–¥—É—â–µ–º —ç—Ç–æ–≥–æ –≥–æ–¥–∞ ‚Äî –±–µ—Ä—ë–º –ø—Ä–æ—à–ª—ã–π –≥–æ–¥
                if month_num > now.month and year == now.year:
                    year -= 1
                
                # –ü–µ—Ä–≤—ã–π –¥–µ–Ω—å –º–µ—Å—è—Ü–∞
                result["date_from"] = datetime(year, month_num, 1)
                # –ü–æ—Å–ª–µ–¥–Ω–∏–π –¥–µ–Ω—å –º–µ—Å—è—Ü–∞
                if month_num == 12:
                    result["date_to"] = datetime(year + 1, 1, 1) - timedelta(days=1)
                else:
                    result["date_to"] = datetime(year, month_num + 1, 1) - timedelta(days=1)
                
                result["has_time_filter"] = True
                result["decay_days"] = (now - result["date_from"]).days or 30
                result["freshness_weight"] = 0.5  # –¢–æ—á–Ω—ã–π –ø–µ—Ä–∏–æ–¥ ‚Äî –≤—ã—Å–æ–∫–∏–π –≤–µ—Å
                break
    
    return result

def search_telegram_chats_sql(query: str, limit: int = 10) -> list:
    """SQL-–ø–æ–∏—Å–∫ –ø–æ —á–∞—Ç–∞–º (—Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Å–ª–æ–≤)."""
    results = []
    conn = get_db_connection()
    keywords = clean_keywords(query)
    try:
        with conn.cursor() as cur:
            cur.execute("""SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name LIKE 'tg_chat_%' AND table_name != 'tg_chats_metadata' AND table_name != 'tg_user_roles'""")
            chat_tables = [row[0] for row in cur.fetchall()]
            for table_name in chat_tables:
                for keyword in keywords[:2]:
                    try:
                        cur.execute(sql.SQL("SELECT timestamp, first_name, message_text, media_analysis, message_type FROM {} WHERE message_text ILIKE %s OR media_analysis ILIKE %s ORDER BY timestamp DESC LIMIT %s").format(sql.Identifier(table_name)), (f"%{keyword}%", f"%{keyword}%", limit))
                        for row in cur.fetchall():
                            chat_name = table_name.replace('tg_chat_', '').split('_', 1)[-1].replace('_', ' ').title()
                            content = row[2] or ""
                            if row[3]:
                                content += f"\n[–ê–Ω–∞–ª–∏–∑]: {row[3][:500]}"
                            result = {"source": f"–ß–∞—Ç: {chat_name}", "date": row[0].strftime("%d.%m.%Y %H:%M") if row[0] else "", "author": row[1] or "", "content": content[:1000], "type": row[4] or "text"}
                            if result not in results:
                                results.append(result)
                    except:
                        continue
    finally:
        conn.close()
    return results[:limit]


def search_telegram_chats_vector(query: str, limit: int = 10, time_context: dict = None) -> list:
    """–í–µ–∫—Ç–æ—Ä–Ω—ã–π (—Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π) –ø–æ–∏—Å–∫ –ø–æ —á–∞—Ç–∞–º —Å —É—á—ë—Ç–æ–º —Å–≤–µ–∂–µ—Å—Ç–∏."""
    if not VECTOR_SEARCH_ENABLED:
        return []
    
    # –ü–æ–ª—É—á–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≤—Ä–µ–º–µ–Ω–∏
    if time_context is None:
        time_context = extract_time_context(query)
    
    decay_days = time_context.get("decay_days", 90)
    freshness_weight = time_context.get("freshness_weight", 0.25)
    
    try:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤–∑–≤–µ—à–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ —Å —É—á—ë—Ç–æ–º —Å–≤–µ–∂–µ—Å—Ç–∏
        vector_results = vector_search_weighted(
            query, 
            limit=limit, 
            source_type='telegram',
            freshness_weight=freshness_weight,
            decay_days=decay_days
        )
        
        results = []
        for r in vector_results:
            chat_name = r['source_table'].replace('tg_chat_', '').split('_', 1)[-1].replace('_', ' ').title()
            
            result = {
                "source": f"–ß–∞—Ç: {chat_name}",
                "content": r['content'][:1000],
                "type": "text",
                "similarity": r.get('similarity', 0),
                "freshness": r.get('freshness', 0),
                "final_score": r.get('final_score', r.get('similarity', 0)),
                "search_type": "vector"
            }
            
            # –î–æ–±–∞–≤–ª—è–µ–º –¥–∞—Ç—É –µ—Å–ª–∏ –µ—Å—Ç—å
            if r.get('timestamp'):
                result["date"] = r['timestamp'].strftime("%d.%m.%Y %H:%M")
            
            results.append(result)
        
        logger.info(f"–í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ (decay={decay_days}d, fw={freshness_weight}): {len(results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
        return results
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞: {e}")
        return []

def search_emails_vector(query: str, limit: int = 10, time_context: dict = None) -> list:
    """–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –ø–æ email —Å —É—á—ë—Ç–æ–º —Å–≤–µ–∂–µ—Å—Ç–∏."""
    if not VECTOR_SEARCH_ENABLED:
        return []
    
    # –ü–æ–ª—É—á–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≤—Ä–µ–º–µ–Ω–∏
    if time_context is None:
        time_context = extract_time_context(query)
    
    decay_days = time_context.get("decay_days", 90)
    freshness_weight = time_context.get("freshness_weight", 0.25)
    
    results = []
    try:
        email_results = vector_search_weighted(
            query, 
            limit=limit, 
            source_type='email',
            freshness_weight=freshness_weight,
            decay_days=decay_days
        )
        
        for r in email_results:
            received_str = ""
            if r.get("received_at"):
                received_str = r["received_at"].strftime("%d.%m.%Y")
            
            results.append({
                "source": "Email",
                "content": r["content"],
                "subject": r.get("subject", ""),
                "from_address": r.get("from_address", ""),
                "date": received_str,
                "similarity": r.get("similarity", 0),
                "freshness": r.get("freshness", 0),
                "final_score": r.get("final_score", r.get("similarity", 0)),
                "search_type": "email_vector"
            })
            
        logger.info(f"Email –ø–æ–∏—Å–∫ (decay={decay_days}d): {len(results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ email: {e}")
    
    return results


def search_telegram_chats(query: str, limit: int = 10, time_context: dict = None) -> list:
    """
    –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ —á–∞—Ç–∞–º:
    1. –í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ (—Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π) ‚Äî –Ω–∞—Ö–æ–¥–∏—Ç –ø–æ —Å–º—ã—Å–ª—É —Å —É—á—ë—Ç–æ–º —Å–≤–µ–∂–µ—Å—Ç–∏
    2. SQL –ø–æ–∏—Å–∫ ‚Äî –Ω–∞—Ö–æ–¥–∏—Ç —Ç–æ—á–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
    3. –û–±—ä–µ–¥–∏–Ω—è–µ–º –∏ –¥–µ–¥—É–ø–ª–∏—Ü–∏—Ä—É–µ–º
    """
    results = []
    seen_content = set()
    
    # –°–Ω–∞—á–∞–ª–∞ –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ (—Å —É—á—ë—Ç–æ–º –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞)
    vector_results = search_telegram_chats_vector(query, limit=limit, time_context=time_context)
    for r in vector_results:
        content_hash = hash(r['content'][:200])
        if content_hash not in seen_content:
            seen_content.add(content_hash)
            results.append(r)
    
    # –ó–∞—Ç–µ–º SQL –ø–æ–∏—Å–∫ –¥–ª—è —Ç–æ—á–Ω—ã—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π
    sql_results = search_telegram_chats_sql(query, limit=limit)
    for r in sql_results:
        content_hash = hash(r['content'][:200])
        if content_hash not in seen_content:
            seen_content.add(content_hash)
            results.append(r)
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ final_score (–µ—Å–ª–∏ –µ—Å—Ç—å) –∏–ª–∏ similarity
    results.sort(key=lambda x: x.get('final_score', x.get('similarity', 0)), reverse=True)
    
    logger.info(f"–ü–æ–∏—Å–∫ –≤ —á–∞—Ç–∞—Ö: {len(results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (vector + sql)")
    return results[:limit]


def search_1c_data(query: str, limit: int = 10) -> list:
    """SQL-–ø–æ–∏—Å–∫ –ø–æ –¥–∞–Ω–Ω—ã–º 1–° (—Ü–µ–Ω—ã, –Ω–æ–º–µ–Ω–∫–ª–∞—Ç—É—Ä–∞, –∫–æ–Ω—Ç—Ä–∞–≥–µ–Ω—Ç—ã)."""
    prices = []
    nomenclature = []
    contractors = []
    
    conn = get_db_connection()
    keywords = clean_keywords(query)
    
    try:
        with conn.cursor() as cur:
            # 1. –ó–ê–ö–£–ü–û–ß–ù–´–ï –¶–ï–ù–´ (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç!)
            for keyword in keywords[:3]:
                try:
                    cur.execute("""
                        SELECT doc_date, doc_number, contractor_name, nomenclature_name, quantity, price, sum_total 
                        FROM purchase_prices 
                        WHERE nomenclature_name ILIKE %s OR contractor_name ILIKE %s 
                        ORDER BY doc_date DESC LIMIT %s
                    """, (f"%{keyword}%", f"%{keyword}%", limit))
                    for row in cur.fetchall():
                        result = {
                            "source": "1–°: –ó–ê–ö–£–ü–û–ß–ù–´–ï –¶–ï–ù–´", 
                            "date": row[0].strftime("%d.%m.%Y") if row[0] else "", 
                            "content": f"{row[3]} –æ—Ç {row[2]}: {row[5]} —Ä—É–±./–µ–¥., –∫–æ–ª-–≤–æ: {row[4]}, —Å—É–º–º–∞: {row[6]} —Ä—É–±. (–¥–æ–∫. {row[1]})", 
                            "type": "price"
                        }
                        if result not in prices:
                            prices.append(result)
                except Exception as e:
                    logger.debug(f"–û—à–∏–±–∫–∞ –∑–∞–∫—É–ø–æ—á–Ω—ã—Ö —Ü–µ–Ω: {e}")
            
            # 2. –ù–æ–º–µ–Ω–∫–ª–∞—Ç—É—Ä–∞ (—Å–ø—Ä–∞–≤–æ—á–Ω–æ)
            for keyword in keywords[:3]:
                try:
                    cur.execute("SELECT name, code, unit FROM nomenclature WHERE name ILIKE %s OR code ILIKE %s LIMIT %s", (f"%{keyword}%", f"%{keyword}%", limit))
                    for row in cur.fetchall():
                        result = {"source": "1–°: –ù–æ–º–µ–Ω–∫–ª–∞—Ç—É—Ä–∞", "content": f"{row[0]} (–∫–æ–¥: {row[1]}, –µ–¥.: {row[2]})", "type": "nomenclature"}
                        if result not in nomenclature:
                            nomenclature.append(result)
                except:
                    pass
            
            # 3. –ö–æ–Ω—Ç—Ä–∞–≥–µ–Ω—Ç—ã (—Å–ø—Ä–∞–≤–æ—á–Ω–æ)
            for keyword in keywords[:3]:
                try:
                    cur.execute("SELECT name, inn, full_name FROM contractors WHERE name ILIKE %s OR inn ILIKE %s LIMIT %s", (f"%{keyword}%", f"%{keyword}%", limit))
                    for row in cur.fetchall():
                        result = {"source": "1–°: –ö–æ–Ω—Ç—Ä–∞–≥–µ–Ω—Ç—ã", "content": f"{row[0]} (–ò–ù–ù: {row[1]})", "type": "contractor"}
                        if result not in contractors:
                            contractors.append(result)
                except:
                    pass
    finally:
        conn.close()
    
    results = prices[:limit]
    remaining = limit - len(results)
    if remaining > 0:
        results.extend(nomenclature[:remaining])
        remaining = limit - len(results)
    if remaining > 0:
        results.extend(contractors[:remaining])
    
    logger.info(f"–ü–æ–∏—Å–∫ 1–° –ø–æ {keywords}: —Ü–µ–Ω—ã={len(prices)}, –Ω–æ–º–µ–Ω–∫–ª–∞—Ç—É—Ä–∞={len(nomenclature)}, –∫–æ–Ω—Ç—Ä–∞–≥–µ–Ω—Ç—ã={len(contractors)}")
    return results[:limit]


def search_internet(query: str) -> tuple:
    """–ü–æ–∏—Å–∫ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ —á–µ—Ä–µ–∑ Perplexity. –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç (—Ç–µ–∫—Å—Ç, —Å–ø–∏—Å–æ–∫_—Å—Å—ã–ª–æ–∫)."""
    if not ROUTERAI_API_KEY:
        return "", []
    try:
        response = requests.post(
            f"{ROUTERAI_BASE_URL}/chat/completions",
            headers={"Authorization": f"Bearer {ROUTERAI_API_KEY}", "Content-Type": "application/json"},
            json={"model": "perplexity/sonar", "messages": [{"role": "user", "content": query}]},
            timeout=60
        )
        result = response.json()
        
        if "choices" not in result:
            return "", []
        
        text = result["choices"][0]["message"]["content"]
        citations = result.get("citations", [])
        
        return text, citations
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç: {e}")
        return "", []


def generate_response(question: str, db_results: list, web_results: str, web_citations: list = None, chat_context: str = "") -> str:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö."""
    if not ROUTERAI_API_KEY:
        return "API –∫–ª—é—á –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω"
    try:
        context_parts = []
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ —Ç–∏–ø—É
        prices = [r for r in db_results if r.get('type') == 'price']
        other_1c = [r for r in db_results if r.get('source', '').startswith('1–°') and r.get('type') != 'price']
        chats = [r for r in db_results if r.get('source', '').startswith('–ß–∞—Ç')]
        emails = [r for r in db_results if r.get('source', '').startswith('Email')]
        
        # –°–Ω–∞—á–∞–ª–∞ –∑–∞–∫—É–ø–æ—á–Ω—ã–µ —Ü–µ–Ω—ã (–ü–†–ò–û–†–ò–¢–ï–¢!)
        if prices:
            context_parts.append("=== –ó–ê–ö–£–ü–û–ß–ù–´–ï –¶–ï–ù–´ –ö–û–ú–ü–ê–ù–ò–ò (–¥–∞–Ω–Ω—ã–µ 1–°) ===")
            for i, res in enumerate(prices, 1):
                context_parts.append(f"{i}. {res.get('date', '')} {res['content']}")
        
        # –ü–æ—Ç–æ–º —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–∏ 1–°
        if other_1c:
            context_parts.append("\n=== –°–ü–†–ê–í–û–ß–ù–ò–ö–ò 1–° ===")
            for i, res in enumerate(other_1c, 1):
                context_parts.append(f"{i}. [{res['source']}] {res['content'][:300]}")
        
        # –ü–æ—Ç–æ–º —á–∞—Ç—ã
        if chats:
            context_parts.append("\n=== –ò–ó –ß–ê–¢–û–í ===")
            for i, res in enumerate(chats[:5], 1):
                score_info = ""
                if 'final_score' in res:
                    score_info = f" [—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {res['final_score']:.0%}]"
                elif 'similarity' in res:
                    score_info = f" [—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {res['similarity']:.0%}]"
                date_info = f" ({res['date']})" if res.get('date') else ""
                context_parts.append(f"{i}.{score_info}{date_info} {res['content'][:300]}")
        
        # –ü–æ—Ç–æ–º email
        if emails:
            context_parts.append("\n=== –ò–ó EMAIL ===")
            for i, res in enumerate(emails[:5], 1):
                score_info = f" [—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {res.get('final_score', res.get('similarity', 0)):.0%}]"
                date_info = f" ({res['date']})" if res.get('date') else ""
                context_parts.append(f"{i}.{score_info}{date_info} {res['content'][:400]}")
        
        # –ò–Ω—Ç–µ—Ä–Ω–µ—Ç
        if web_results:
            context_parts.append("\n=== –ò–ù–¢–ï–†–ù–ï–¢ ===")
            context_parts.append(web_results[:2000])
        
        context = "\n".join(context_parts)
        
        prompt = f"""–¢—ã ‚Äî RAG-–∞–≥–µ–Ω—Ç –∫–æ–º–ø–∞–Ω–∏–∏. –û—Ç–≤–µ—á–∞–π –Ω–∞ —Ä—É—Å—Å–∫–æ–º.

–í–û–ü–†–û–°: {question}

–ù–ê–ô–î–ï–ù–ù–´–ï –î–ê–ù–ù–´–ï:
{context if context else "–ù–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ."}

–í–ê–ñ–ù–´–ï –ò–ù–°–¢–†–£–ö–¶–ò–ò:
1. –ó–ê–ö–£–ü–û–ß–ù–´–ï –¶–ï–ù–´ –ö–û–ú–ü–ê–ù–ò–ò ‚Äî —ç—Ç–æ –†–ï–ê–õ–¨–ù–´–ï —Ü–µ–Ω—ã –ø–æ –∫–æ—Ç–æ—Ä—ã–º –º—ã –ø–æ–∫—É–ø–∞–µ–º —Ç–æ–≤–∞—Ä. –í–°–ï–ì–î–ê —É–∫–∞–∑—ã–≤–∞–π –∏—Ö –≤ –æ—Ç–≤–µ—Ç–µ!
2. –ï—Å–ª–∏ —Å–ø—Ä–∞—à–∏–≤–∞—é—Ç –æ —Ü–µ–Ω–µ "—É –Ω–∞—Å" ‚Äî —ç—Ç–æ –∑–∞–∫—É–ø–æ—á–Ω—ã–µ —Ü–µ–Ω—ã –∏–∑ 1–°
3. –ï—Å–ª–∏ —Å–ø—Ä–∞—à–∏–≤–∞—é—Ç –æ —Ä—ã–Ω–æ—á–Ω—ã—Ö —Ü–µ–Ω–∞—Ö ‚Äî –∏—Å–ø–æ–ª—å–∑—É–π –¥–∞–Ω–Ω—ã–µ –∏–∑ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞
4. –î–∞–Ω–Ω—ã–µ –∏–∑ –ß–ê–¢–û–í –∏ EMAIL ‚Äî —ç—Ç–æ –≤–Ω—É—Ç—Ä–µ–Ω–Ω—è—è –ø–µ—Ä–µ–ø–∏—Å–∫–∞ –∫–æ–º–ø–∞–Ω–∏–∏
5. –£–∫–∞–∑—ã–≤–∞–π –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ü–∏—Ñ—Ä—ã: —Ü–µ–Ω—É, –¥–∞—Ç—É, –ø–æ—Å—Ç–∞–≤—â–∏–∫–∞
6. –ù–µ –ø—Ä–∏–¥—É–º—ã–≤–∞–π –¥–∞–Ω–Ω—ã–µ

–û—Ç–≤–µ—Ç:"""

        response = requests.post(f"{ROUTERAI_BASE_URL}/chat/completions", headers={"Authorization": f"Bearer {ROUTERAI_API_KEY}", "Content-Type": "application/json"}, json={"model": "google/gemini-3-flash-preview", "messages": [{"role": "user", "content": prompt}], "max_tokens": 2000}, timeout=60)
        result = response.json()
        if "choices" in result:
            response_text = result["choices"][0]["message"]["content"]
            
            if web_citations:
                response_text += "\n\nüìé **–ò—Å—Ç–æ—á–Ω–∏–∫–∏:**"
                for i, url in enumerate(web_citations[:5], 1):
                    response_text += f"\n{i}. {url}"
            
            return response_text
        return "–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏"
    except Exception as e:
        return f"–û—à–∏–±–∫–∞: {e}"


def classify_question(question: str) -> dict:
    """–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –≤–æ–ø—Ä–æ—Å–∞ –¥–ª—è –≤—ã–±–æ—Ä–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –ø–æ–∏—Å–∫–∞."""
    if not ROUTERAI_API_KEY:
        return {"search_1c": True, "search_chats": True, "search_email": True, "search_web": False, "keywords": question, "priority": "1c"}
    try:
        prompt = f"""–û–ø—Ä–µ–¥–µ–ª–∏ –≥–¥–µ –∏—Å–∫–∞—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é.
–ò—Å—Ç–æ—á–Ω–∏–∫–∏: 1–° (—Ü–µ–Ω—ã, –∑–∞–∫—É–ø–∫–∏, —Ç–æ–≤–∞—Ä—ã), –ß–∞—Ç—ã (–æ–±—Å—É–∂–¥–µ–Ω–∏—è –≤ Telegram), Email (–ø–µ—Ä–µ–ø–∏—Å–∫–∞ –ø–æ –ø–æ—á—Ç–µ), –ò–Ω—Ç–µ—Ä–Ω–µ—Ç (–≤–Ω–µ—à–Ω—è—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è).
–ò–∑–≤–ª–µ–∫–∏ 1-3 –ö–õ–Æ–ß–ï–í–´–• –°–õ–û–í–ê (—Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–µ –±–µ–∑ –∑–∞–ø—è—Ç—ã—Ö: —Å–∞—Ö–∞—Ä –º—É–∫–∞ —Ç–æ—Ä—Ç)

–í–æ–ø—Ä–æ—Å: {question}

JSON: {{"search_1c": true/false, "search_chats": true/false, "search_email": true/false, "search_web": true/false, "keywords": "—Å–ª–æ–≤–æ1 —Å–ª–æ–≤–æ2", "priority": "1c/chats/email/web"}}"""
        response = requests.post(f"{ROUTERAI_BASE_URL}/chat/completions", headers={"Authorization": f"Bearer {ROUTERAI_API_KEY}", "Content-Type": "application/json"}, json={"model": "google/gemini-3-flash-preview", "messages": [{"role": "user", "content": prompt}], "max_tokens": 200}, timeout=30)
        result = response.json()
        if "choices" in result:
            match = re.search(r'\{[^}]+\}', result["choices"][0]["message"]["content"])
            if match:
                return json.loads(match.group())
        return {"search_1c": True, "search_chats": True, "search_email": True, "search_web": False, "keywords": question, "priority": "1c"}
    except:
        return {"search_1c": True, "search_chats": True, "search_email": True, "search_web": False, "keywords": question, "priority": "1c"}


async def process_rag_query(question: str, chat_context: str = "") -> str:
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ RAG-–∑–∞–ø—Ä–æ—Å–∞ —Å —É—á—ë—Ç–æ–º –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞."""
    logger.info(f"RAG –∑–∞–ø—Ä–æ—Å: {question}")
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω–æ–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –≤–æ–ø—Ä–æ—Å–∞
    time_context = extract_time_context(question)
    if time_context["has_time_filter"]:
        logger.info(f"–í—Ä–µ–º–µ–Ω–Ω–æ–π –∫–æ–Ω—Ç–µ–∫—Å—Ç: decay_days={time_context['decay_days']}, fw={time_context['freshness_weight']}")
    
    # –ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ–º –≤–æ–ø—Ä–æ—Å
    classification = classify_question(question)
    logger.info(f"–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è: {classification}")
    
    keywords = classification.get("keywords", question)
    db_results = []
    
    # –ü–æ–∏—Å–∫ –≤ 1–° (SQL) ‚Äî –≤—Å–µ–≥–¥–∞ –ø–µ—Ä–≤—ã–º
    if classification.get("search_1c", True):
        c1_results = search_1c_data(keywords, limit=15)
        db_results.extend(c1_results)
        logger.info(f"–ù–∞–π–¥–µ–Ω–æ –≤ 1–°: {len(c1_results)}")
    
    # –ü–æ–∏—Å–∫ –≤ —á–∞—Ç–∞—Ö (–≤–µ–∫—Ç–æ—Ä–Ω—ã–π —Å —É—á—ë—Ç–æ–º —Å–≤–µ–∂–µ—Å—Ç–∏ + SQL)
    if classification.get("search_chats", True):
        chat_results = search_telegram_chats(keywords, limit=10, time_context=time_context)
        db_results.extend(chat_results)
        logger.info(f"–ù–∞–π–¥–µ–Ω–æ –≤ —á–∞—Ç–∞—Ö: {len(chat_results)}")
    
    # –ü–æ–∏—Å–∫ –≤ email (–≤–µ–∫—Ç–æ—Ä–Ω—ã–π —Å —É—á—ë—Ç–æ–º —Å–≤–µ–∂–µ—Å—Ç–∏)
    if classification.get("search_email", True):
        email_results = search_emails_vector(keywords, limit=10, time_context=time_context)
        db_results.extend(email_results)
        logger.info(f"–ù–∞–π–¥–µ–Ω–æ –≤ email: {len(email_results)}")
    
    logger.info(f"–í—Å–µ–≥–æ –≤ –ë–î: {len(db_results)}")
    
    # –ü–æ–∏—Å–∫ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ
    web_results = ""
    web_citations = []
    if classification.get("search_web", False):
        web_results, web_citations = search_internet(question)
    
    return generate_response(question, db_results, web_results, web_citations, chat_context)


async def index_new_message(table_name: str, message_id: int, content: str):
    """–ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ—Ç –Ω–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –¥–ª—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞."""
    if not VECTOR_SEARCH_ENABLED:
        return
    
    if not content or len(content.strip()) < 10:
        return
    
    try:
        index_telegram_message(table_name, message_id, content)
        logger.debug(f"–ü—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–æ —Å–æ–æ–±—â–µ–Ω–∏–µ {message_id} –∏–∑ {table_name}")
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ —Å–æ–æ–±—â–µ–Ω–∏—è: {e}")
