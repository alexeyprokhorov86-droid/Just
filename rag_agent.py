"""
RAG Agent Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¿Ğ¾ Ğ±Ğ°Ğ·Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ½ĞµÑ‚Ñƒ.
Ğ’ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ SQL-Ğ¿Ğ¾Ğ¸ÑĞº Ğ¸ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ñ‹Ğ¹ (ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹) Ğ¿Ğ¾Ğ¸ÑĞº Ñ ÑƒÑ‡Ñ‘Ñ‚Ğ¾Ğ¼ ÑĞ²ĞµĞ¶ĞµÑÑ‚Ğ¸.
"""

import os
import pathlib
from dotenv import load_dotenv
from company_context import get_company_profile

env_path = pathlib.Path(__file__).parent / '.env'
load_dotenv(dotenv_path=env_path if env_path.exists() else None)

import json
import logging
import requests
import psycopg2
from psycopg2 import sql
import re
from datetime import datetime, timedelta

logger = logging.getLogger(__name__)

DB_HOST = os.getenv("DB_HOST", "localhost")
DB_PORT = os.getenv("DB_PORT", "5432")
DB_NAME = os.getenv("DB_NAME", "knowledge_base")
DB_USER = os.getenv("DB_USER", "knowledge")
DB_PASSWORD = os.getenv("DB_PASSWORD")
ROUTERAI_API_KEY = os.getenv("ROUTERAI_API_KEY")
ROUTERAI_BASE_URL = os.getenv("ROUTERAI_BASE_URL", "https://routerai.ru/api/v1")

# Ğ˜Ğ¼Ğ¿Ğ¾Ñ€Ñ‚ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ°
try:
    from embedding_service import vector_search, vector_search_weighted, index_telegram_message
    VECTOR_SEARCH_ENABLED = True
    logger.info("Ğ’ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ²ĞºĞ»ÑÑ‡ĞµĞ½")
except ImportError:
    VECTOR_SEARCH_ENABLED = False
    logger.warning("embedding_service Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½, Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ¾Ñ‚ĞºĞ»ÑÑ‡ĞµĞ½")


def get_db_connection():
    return psycopg2.connect(host=DB_HOST, port=DB_PORT, dbname=DB_NAME, user=DB_USER, password=DB_PASSWORD)


def clean_keywords(query: str) -> list:
    """ĞÑ‡Ğ¸Ñ‰Ğ°ĞµÑ‚ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ ÑĞ»Ğ¾Ğ²Ğ° Ğ¾Ñ‚ Ğ¿ÑƒĞ½ĞºÑ‚ÑƒĞ°Ñ†Ğ¸Ğ¸."""
    clean_query = re.sub(r'[,.:;!?()"\']', ' ', query)
    keywords = [w.strip() for w in clean_query.split() if len(w.strip()) > 2]
    return keywords if keywords else [query]

def extract_time_context(question: str) -> dict:
    """
    Ğ˜Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµÑ‚ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ¸Ğ· Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°.
    
    Ğ•ÑĞ»Ğ¸ Ğ² Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞµ ÑƒĞºĞ°Ğ·Ğ°Ğ½ Ğ¿ĞµÑ€Ğ¸Ğ¾Ğ´ (Ğ·Ğ° Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½Ğ¸Ğ¹ Ğ¼ĞµÑÑÑ†, Ğ²Ñ‡ĞµÑ€Ğ°, Ğ² ÑĞ½Ğ²Ğ°Ñ€Ğµ) â€”
    Ğ½Ğ°ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¿Ğ¾Ğ´ ÑÑ‚Ğ¾Ñ‚ Ğ¿ĞµÑ€Ğ¸Ğ¾Ğ´.
    
    Ğ•ÑĞ»Ğ¸ Ğ¿ĞµÑ€Ğ¸Ğ¾Ğ´ Ğ½Ğµ ÑƒĞºĞ°Ğ·Ğ°Ğ½ â€” Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ decay_days=90 Ğ¿Ğ¾ ÑƒĞ¼Ğ¾Ğ»Ñ‡Ğ°Ğ½Ğ¸Ñ.
    """
    question_lower = question.lower()
    now = datetime.now()
    
    result = {
        "has_time_filter": False,
        "date_from": None,
        "date_to": None,
        "decay_days": 90,  # ĞŸĞ¾ ÑƒĞ¼Ğ¾Ğ»Ñ‡Ğ°Ğ½Ğ¸Ñ 90 Ğ´Ğ½ĞµĞ¹
        "freshness_weight": 0.25  # ĞŸĞ¾ ÑƒĞ¼Ğ¾Ğ»Ñ‡Ğ°Ğ½Ğ¸Ñ
    }
    
    # ĞŸĞ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹ Ğ´Ğ»Ñ "Ğ·Ğ° Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½Ğ¸Ğ¹/Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½Ğ¸Ğµ N Ğ´Ğ½ĞµĞ¹/Ğ½ĞµĞ´ĞµĞ»ÑŒ/Ğ¼ĞµÑÑÑ†ĞµĞ²"
    patterns = [
        # "Ğ·Ğ° Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½Ğ¸Ğ¹ Ğ¼ĞµÑÑÑ†", "Ğ·Ğ° Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½Ğ¸Ğµ 2 Ğ¼ĞµÑÑÑ†Ğ°"
        (r'Ğ·Ğ° Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½(?:Ğ¸Ğ¹|Ğ¸Ğµ|ÑÑ|ĞµĞµ)?\s*(\d+)?\s*Ğ¼ĞµÑÑÑ†', lambda m: int(m.group(1) or 1) * 30),
        (r'Ğ·Ğ° (\d+)\s*Ğ¼ĞµÑÑÑ†', lambda m: int(m.group(1)) * 30),
        
        # "Ğ·Ğ° Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½ÑÑ Ğ½ĞµĞ´ĞµĞ»Ñ", "Ğ·Ğ° Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½Ğ¸Ğµ 2 Ğ½ĞµĞ´ĞµĞ»Ğ¸"  
        (r'Ğ·Ğ° Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½(?:Ğ¸Ğ¹|Ğ¸Ğµ|ÑÑ|ĞµĞµ)?\s*(\d+)?\s*Ğ½ĞµĞ´ĞµĞ»', lambda m: int(m.group(1) or 1) * 7),
        (r'Ğ·Ğ° (\d+)\s*Ğ½ĞµĞ´ĞµĞ»', lambda m: int(m.group(1)) * 7),
        
        # "Ğ·Ğ° Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½Ğ¸Ğ¹ Ğ´ĞµĞ½ÑŒ", "Ğ·Ğ° Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½Ğ¸Ğµ 3 Ğ´Ğ½Ñ"
        (r'Ğ·Ğ° Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½(?:Ğ¸Ğ¹|Ğ¸Ğµ|ÑÑ|ĞµĞµ)?\s*(\d+)?\s*(?:Ğ´ĞµĞ½ÑŒ|Ğ´Ğ½Ñ|Ğ´Ğ½ĞµĞ¹)', lambda m: int(m.group(1) or 1)),
        (r'Ğ·Ğ° (\d+)\s*(?:Ğ´ĞµĞ½ÑŒ|Ğ´Ğ½Ñ|Ğ´Ğ½ĞµĞ¹)', lambda m: int(m.group(1))),
        
        # "Ğ·Ğ° Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½Ğ¸Ğ¹ Ğ³Ğ¾Ğ´"
        (r'Ğ·Ğ° Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½(?:Ğ¸Ğ¹|Ğ¸Ğµ|ÑÑ|ĞµĞµ)?\s*Ğ³Ğ¾Ğ´', lambda m: 365),
        (r'Ğ·Ğ° Ğ³Ğ¾Ğ´', lambda m: 365),
        
        # "Ğ·Ğ° Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½Ğ¸Ğ¹ ĞºĞ²Ğ°Ñ€Ñ‚Ğ°Ğ»"
        (r'Ğ·Ğ° Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½(?:Ğ¸Ğ¹|Ğ¸Ğµ|ÑÑ|ĞµĞµ)?\s*ĞºĞ²Ğ°Ñ€Ñ‚Ğ°Ğ»', lambda m: 90),
        (r'Ğ·Ğ° ĞºĞ²Ğ°Ñ€Ñ‚Ğ°Ğ»', lambda m: 90),
        
        # "Ğ²Ñ‡ĞµÑ€Ğ°"
        (r'\bĞ²Ñ‡ĞµÑ€Ğ°\b', lambda m: 2),
        
        # "ÑĞµĞ³Ğ¾Ğ´Ğ½Ñ"
        (r'\bÑĞµĞ³Ğ¾Ğ´Ğ½Ñ\b', lambda m: 1),
        
        # "Ğ½Ğ° ÑÑ‚Ğ¾Ğ¹ Ğ½ĞµĞ´ĞµĞ»Ğµ"
        (r'Ğ½Ğ° ÑÑ‚Ğ¾Ğ¹ Ğ½ĞµĞ´ĞµĞ»Ğµ', lambda m: 7),
        (r'Ğ½Ğ° Ğ¿Ñ€Ğ¾ÑˆĞ»Ğ¾Ğ¹ Ğ½ĞµĞ´ĞµĞ»Ğµ', lambda m: 14),
        
        # "Ğ² ÑÑ‚Ğ¾Ğ¼ Ğ¼ĞµÑÑÑ†Ğµ"
        (r'Ğ² ÑÑ‚Ğ¾Ğ¼ Ğ¼ĞµÑÑÑ†Ğµ', lambda m: now.day),
        (r'Ğ² Ğ¿Ñ€Ğ¾ÑˆĞ»Ğ¾Ğ¼ Ğ¼ĞµÑÑÑ†Ğµ', lambda m: 60),
        
        # "Ğ½ĞµĞ´Ğ°Ğ²Ğ½Ğ¾" - Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ 14 Ğ´Ğ½ĞµĞ¹
        (r'\bĞ½ĞµĞ´Ğ°Ğ²Ğ½Ğ¾\b', lambda m: 14),
        
        # "Ğ² Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½ĞµĞµ Ğ²Ñ€ĞµĞ¼Ñ" - 30 Ğ´Ğ½ĞµĞ¹
        (r'Ğ² Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½ĞµĞµ Ğ²Ñ€ĞµĞ¼Ñ', lambda m: 30),
    ]
    
    for pattern, days_func in patterns:
        match = re.search(pattern, question_lower)
        if match:
            result["has_time_filter"] = True
            result["decay_days"] = days_func(match)
            result["date_from"] = now - timedelta(days=result["decay_days"])
            result["date_to"] = now
            # Ğ•ÑĞ»Ğ¸ ÑƒĞºĞ°Ğ·Ğ°Ğ½ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğ¹ Ğ¿ĞµÑ€Ğ¸Ğ¾Ğ´ â€” ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ²Ğ°ĞµĞ¼ Ğ²ĞµÑ ÑĞ²ĞµĞ¶ĞµÑÑ‚Ğ¸
            result["freshness_weight"] = 0.4
            break
    
    # ĞŸĞ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹ Ğ´Ğ»Ñ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ¼ĞµÑÑÑ†ĞµĞ²: "Ğ² ÑĞ½Ğ²Ğ°Ñ€Ğµ", "Ğ² ÑĞ½Ğ²Ğ°Ñ€Ğµ 2025"
    months = {
        'ÑĞ½Ğ²Ğ°Ñ€': 1, 'Ñ„ĞµĞ²Ñ€Ğ°Ğ»': 2, 'Ğ¼Ğ°Ñ€Ñ‚': 3, 'Ğ°Ğ¿Ñ€ĞµĞ»': 4,
        'Ğ¼Ğ°Ğµ': 5, 'Ğ¼Ğ°Ñ': 5, 'Ğ¼Ğ°Ğ¹': 5, 'Ğ¸ÑĞ½': 6, 'Ğ¸ÑĞ»': 7, 'Ğ°Ğ²Ğ³ÑƒÑÑ‚': 8,
        'ÑĞµĞ½Ñ‚ÑĞ±Ñ€': 9, 'Ğ¾ĞºÑ‚ÑĞ±Ñ€': 10, 'Ğ½Ğ¾ÑĞ±Ñ€': 11, 'Ğ´ĞµĞºĞ°Ğ±Ñ€': 12
    }
    
    if not result["has_time_filter"]:
        for month_pattern, month_num in months.items():
            match = re.search(rf'Ğ²\s+{month_pattern}\w*\s*(\d{{4}})?', question_lower)
            if match:
                year = int(match.group(1)) if match.group(1) else now.year
                # Ğ•ÑĞ»Ğ¸ Ğ¼ĞµÑÑÑ† Ğ² Ğ±ÑƒĞ´ÑƒÑ‰ĞµĞ¼ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ³Ğ¾Ğ´Ğ° â€” Ğ±ĞµÑ€Ñ‘Ğ¼ Ğ¿Ñ€Ğ¾ÑˆĞ»Ñ‹Ğ¹ Ğ³Ğ¾Ğ´
                if month_num > now.month and year == now.year:
                    year -= 1
                
                # ĞŸĞµÑ€Ğ²Ñ‹Ğ¹ Ğ´ĞµĞ½ÑŒ Ğ¼ĞµÑÑÑ†Ğ°
                result["date_from"] = datetime(year, month_num, 1)
                # ĞŸĞ¾ÑĞ»ĞµĞ´Ğ½Ğ¸Ğ¹ Ğ´ĞµĞ½ÑŒ Ğ¼ĞµÑÑÑ†Ğ°
                if month_num == 12:
                    result["date_to"] = datetime(year + 1, 1, 1) - timedelta(days=1)
                else:
                    result["date_to"] = datetime(year, month_num + 1, 1) - timedelta(days=1)
                
                result["has_time_filter"] = True
                result["decay_days"] = (now - result["date_from"]).days or 30
                result["freshness_weight"] = 0.5  # Ğ¢Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ Ğ¿ĞµÑ€Ğ¸Ğ¾Ğ´ â€” Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ğ¹ Ğ²ĞµÑ
                break
    
    return result

def diversify_by_source_id(
    items: list,
    total_limit: int,
    max_per_source: int = 2,
    score_key: str = "final_score",
    source_id_key: str = "source_id",
) -> list:
    """
    ĞĞ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ‡Ğ¸ÑĞ»Ğ¾ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¾Ñ‚ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ° (source_id).
    ĞÑƒĞ¶Ğ½Ğ° Ğ´Ğ»Ñ email, Ğ³Ğ´Ğµ Ğ¾Ğ´Ğ¸Ğ½ email = Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ñ‡Ğ°Ğ½ĞºĞ¾Ğ² => Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¿Ğ°Ğ´Ğ°Ğ½Ğ¸Ğ¹ Ğ¸Ğ· Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¸ÑÑŒĞ¼Ğ°.

    Ğ›Ğ¾Ğ³Ğ¸ĞºĞ°:
    - Ğ¾Ğ¶Ğ¸Ğ´Ğ°ĞµĞ¼, Ñ‡Ñ‚Ğ¾ items ÑƒĞ¶Ğµ Ğ¾Ñ‚ÑĞ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ñ‹ Ğ¿Ğ¾ score desc (Ğ¸Ğ»Ğ¸ Ğ¼Ñ‹ ÑĞ¾Ñ€Ñ‚Ğ¸Ñ€ÑƒĞµĞ¼ ÑĞ°Ğ¼Ğ¸)
    - Ğ±ĞµÑ€Ñ‘Ğ¼ Ğ¿Ğ¾ max_per_source Ğ½Ğ° Ğ¾Ğ´Ğ¸Ğ½ source_id
    - Ğ¾ÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµĞ¼ÑÑ Ğ½Ğ° total_limit
    """
    if not items:
        return []

    # ĞĞ° Ğ²ÑÑĞºĞ¸Ğ¹ ÑĞ»ÑƒÑ‡Ğ°Ğ¹ ÑĞ¾Ñ€Ñ‚Ğ¸Ñ€ÑƒĞµĞ¼ (Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ½Ğµ Ğ·Ğ°Ğ²Ğ¸ÑĞµÑ‚ÑŒ Ğ¾Ñ‚ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ‘Ğ”/Ğ¸Ğ½Ğ´ĞµĞºÑĞ°)
    items = sorted(items, key=lambda x: x.get(score_key, 0), reverse=True)

    per_source_count = {}
    out = []

    for it in items:
        sid = it.get(source_id_key)
        if sid is None:
            # ĞµÑĞ»Ğ¸ source_id Ğ¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ğ²ÑƒĞµÑ‚ â€” ÑÑ‡Ğ¸Ñ‚Ğ°ĞµĞ¼ ĞºĞ°Ğº ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹
            out.append(it)
            if len(out) >= total_limit:
                break
            continue

        cnt = per_source_count.get(sid, 0)
        if cnt >= max_per_source:
            continue

        per_source_count[sid] = cnt + 1
        out.append(it)

        if len(out) >= total_limit:
            break

    return out

def search_telegram_chats_sql(query: str, limit: int = 30) -> list:
    """SQL-Ğ¿Ğ¾Ğ¸ÑĞº Ğ¿Ğ¾ Ñ‡Ğ°Ñ‚Ğ°Ğ¼ (Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğµ ÑĞ¾Ğ²Ğ¿Ğ°Ğ´ĞµĞ½Ğ¸Ğµ ÑĞ»Ğ¾Ğ²)."""
    results = []
    conn = get_db_connection()
    keywords = clean_keywords(query)
    try:
        with conn.cursor() as cur:
            cur.execute("""SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name LIKE 'tg_chat_%' AND table_name != 'tg_chats_metadata' AND table_name != 'tg_user_roles'""")
            chat_tables = [row[0] for row in cur.fetchall()]
            for table_name in chat_tables:
                for keyword in keywords[:2]:
                    try:
                        cur.execute(sql.SQL("SELECT timestamp, first_name, message_text, media_analysis, message_type FROM {} WHERE message_text ILIKE %s OR media_analysis ILIKE %s ORDER BY timestamp DESC LIMIT %s").format(sql.Identifier(table_name)), (f"%{keyword}%", f"%{keyword}%", limit))
                        for row in cur.fetchall():
                            chat_name = table_name.replace('tg_chat_', '').split('_', 1)[-1].replace('_', ' ').title()
                            content = row[2] or ""
                            if row[3]:
                                content += f"\n[ĞĞ½Ğ°Ğ»Ğ¸Ğ·]: {row[3][:500]}"
                            result = {"source": f"Ğ§Ğ°Ñ‚: {chat_name}", "date": row[0].strftime("%d.%m.%Y %H:%M") if row[0] else "", "author": row[1] or "", "content": content[:1000], "type": row[4] or "text"}
                            if result not in results:
                                results.append(result)
                    except:
                        continue
    finally:
        conn.close()
    return results[:limit]


def search_telegram_chats_vector(query: str, limit: int = 30, time_context: dict = None) -> list:
    """Ğ’ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ñ‹Ğ¹ (ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹) Ğ¿Ğ¾Ğ¸ÑĞº Ğ¿Ğ¾ Ñ‡Ğ°Ñ‚Ğ°Ğ¼ Ñ ÑƒÑ‡Ñ‘Ñ‚Ğ¾Ğ¼ ÑĞ²ĞµĞ¶ĞµÑÑ‚Ğ¸."""
    if not VECTOR_SEARCH_ENABLED:
        return []
    
    # ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸
    if time_context is None:
        time_context = extract_time_context(query)
    
    decay_days = time_context.get("decay_days", 90)
    freshness_weight = time_context.get("freshness_weight", 0.25)
    
    try:
        # Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ Ğ²Ğ·Ğ²ĞµÑˆĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ñ ÑƒÑ‡Ñ‘Ñ‚Ğ¾Ğ¼ ÑĞ²ĞµĞ¶ĞµÑÑ‚Ğ¸
        vector_results = vector_search_weighted(
            query, 
            limit=limit, 
            source_type='telegram',
            freshness_weight=freshness_weight,
            decay_days=decay_days
        )
        
        results = []
        for r in vector_results:
            chat_name = r['source_table'].replace('tg_chat_', '').split('_', 1)[-1].replace('_', ' ').title()
            
            result = {
                "source": f"Ğ§Ğ°Ñ‚: {chat_name}",
                "content": r['content'][:1000],
                "type": "text",
                "similarity": r.get('similarity', 0),
                "freshness": r.get('freshness', 0),
                "final_score": r.get('final_score', r.get('similarity', 0)),
                "search_type": "vector"
            }
            
            # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ´Ğ°Ñ‚Ñƒ ĞµÑĞ»Ğ¸ ĞµÑÑ‚ÑŒ
            if r.get('timestamp'):
                result["date"] = r['timestamp'].strftime("%d.%m.%Y %H:%M")
            
            results.append(result)
        
        logger.info(f"Ğ’ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº (decay={decay_days}d, fw={freshness_weight}): {len(results)} Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ²")
        return results
        
    except Exception as e:
        logger.error(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ°: {e}")
        return []

def search_emails_sql(query: str, limit: int = 30) -> list:
    """SQL/keyword Ğ¿Ğ¾Ğ¸ÑĞº Ğ¿Ğ¾ email â€” Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… ÑĞ¾Ğ²Ğ¿Ğ°Ğ´ĞµĞ½Ğ¸Ğ¹ (Ğ°Ñ€Ñ‚Ğ¸ĞºÑƒĞ»Ñ‹, Ğ½Ğ¾Ğ¼ĞµÑ€Ğ°, Ğ˜ĞĞ)."""
    results = []
    conn = get_db_connection()
    keywords = clean_keywords(query)
    
    try:
        with conn.cursor() as cur:
            # FTS Ğ¿Ğ¾Ğ¸ÑĞº
            fts_query = ' | '.join(keywords[:3])
            cur.execute("""
                SELECT id, subject, body_text, from_address, received_at
                FROM email_messages
                WHERE to_tsvector('russian', COALESCE(subject, '') || ' ' || COALESCE(body_text, ''))
                      @@ to_tsquery('russian', %s)
                ORDER BY received_at DESC
                LIMIT %s
            """, (fts_query, limit * 2))
            
            fts_results = cur.fetchall()
            
            # Ğ•ÑĞ»Ğ¸ FTS Ğ½Ğµ Ğ´Ğ°Ğ» Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² â€” ILIKE fallback
            if not fts_results:
                for keyword in keywords[:2]:
                    cur.execute("""
                        SELECT id, subject, body_text, from_address, received_at
                        FROM email_messages
                        WHERE subject ILIKE %s OR body_text ILIKE %s
                        ORDER BY received_at DESC
                        LIMIT %s
                    """, (f"%{keyword}%", f"%{keyword}%", limit))
                    fts_results.extend(cur.fetchall())
            
            seen_ids = set()
            for row in fts_results:
                if row[0] in seen_ids:
                    continue
                seen_ids.add(row[0])
                
                content = f"Ğ¢ĞµĞ¼Ğ°: {row[1] or ''}\n{(row[2] or '')[:800]}"
                received_str = row[4].strftime("%d.%m.%Y") if row[4] else ""
                
                results.append({
                    "source": "Email",
                    "content": content,
                    "subject": row[1] or "",
                    "from_address": row[3] or "",
                    "date": received_str,
                    "similarity": 0.5,
                    "final_score": 0.5,
                    "search_type": "email_sql",
                    "source_id": row[0],
                })
                
                if len(results) >= limit:
                    break
                    
    except Exception as e:
        logger.error(f"ĞÑˆĞ¸Ğ±ĞºĞ° SQL Ğ¿Ğ¾Ğ¸ÑĞºĞ° email: {e}")
    finally:
        conn.close()
    
    logger.info(f"Email SQL Ğ¿Ğ¾Ğ¸ÑĞº: {len(results)} Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ²")
    return results

def search_emails_vector(query: str, limit: int = 30, time_context: dict = None) -> list:
    """Ğ¡ĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ¿Ğ¾ email Ñ ÑƒÑ‡Ñ‘Ñ‚Ğ¾Ğ¼ ÑĞ²ĞµĞ¶ĞµÑÑ‚Ğ¸ + diversity Ğ¿Ğ¾ source_id (Ñ‡Ğ°Ğ½ĞºĞ¸ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¸ÑÑŒĞ¼Ğ°)."""
    if not VECTOR_SEARCH_ENABLED:
        return []

    if time_context is None:
        time_context = extract_time_context(query)

    decay_days = time_context.get("decay_days", 90)
    freshness_weight = time_context.get("freshness_weight", 0.25)

    # Ğ¡ĞºĞ¾Ğ»ÑŒĞºĞ¾ ĞºĞ°Ğ½Ğ´Ğ¸Ğ´Ğ°Ñ‚Ğ¾Ğ² Ğ²Ğ·ÑÑ‚ÑŒ Ğ´Ğ¾ Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸:
    #  - ĞµÑĞ»Ğ¸ max_per_email=2 Ğ¸ Ğ½ÑƒĞ¶Ğ½Ğ¾ limit=10, Ñ‚Ğ¾ ĞºĞ°Ğ½Ğ´Ğ¸Ğ´Ğ°Ñ‚Ğ¾Ğ² Ğ»ÑƒÑ‡ÑˆĞµ 50-80,
    #    Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¿Ğ¾ÑĞ»Ğµ Ğ¾Ñ‚Ğ±Ñ€Ğ°ÑÑ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´ÑƒĞ±Ğ»ĞµĞ¹ Ğ½Ğµ Ğ¾ÑÑ‚Ğ°Ñ‚ÑŒÑÑ Ğ±ĞµĞ· Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ².
    pre_limit = max(limit * 6, 50)
    max_chunks_per_email = 2

    results = []
    try:
        email_candidates = vector_search_weighted(
            query,
            limit=pre_limit,
            source_type='email',
            freshness_weight=freshness_weight,
            decay_days=decay_days
        )

        # ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ¹ ÑˆĞ°Ğ³ Ğ¿ÑƒĞ½ĞºÑ‚Ğ° 1: Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°ĞµĞ¼ Ñ‡Ğ°Ğ½ĞºĞ¸ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¸ÑÑŒĞ¼Ğ°
        diversified = diversify_by_source_id(
            email_candidates,
            total_limit=limit,
            max_per_source=max_chunks_per_email,
            score_key="final_score",
            source_id_key="source_id",
        )

        for r in diversified:
            received_str = ""
            if r.get("received_at"):
                received_str = r["received_at"].strftime("%d.%m.%Y")

            results.append({
                "source": "Email",
                "content": r.get("content", ""),
                "subject": r.get("subject", ""),
                "from_address": r.get("from_address", ""),
                "date": received_str,
                "similarity": r.get("similarity", 0),
                "freshness": r.get("freshness", 0),
                "final_score": r.get("final_score", r.get("similarity", 0)),
                "search_type": "email_vector",
                # Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ğ¾ Ğ´Ğ»Ñ Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞ¸Ñ… ÑˆĞ°Ğ³Ğ¾Ğ² Ğ¸ Ğ¾Ñ‚Ğ»Ğ°Ğ´ĞºĞ¸
                "source_id": r.get("source_id"),
            })

        logger.info(
            f"Email vector search: pre_limit={pre_limit}, diversified={len(results)} "
            f"(max_per_email={max_chunks_per_email}, decay={decay_days}d, fw={freshness_weight})"
        )

    except Exception as e:
        logger.error(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ğ¾Ğ¸ÑĞºĞ° email: {e}")

    return results

def search_emails(query: str, limit: int = 30, time_context: dict = None) -> list:
    """
    ĞšĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ¿Ğ¾ email:
    1. Ğ’ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº (ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹) â€” Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ğ¾ ÑĞ¼Ñ‹ÑĞ»Ñƒ
    2. SQL Ğ¿Ğ¾Ğ¸ÑĞº â€” Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¸Ñ‚ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ ÑĞ¾Ğ²Ğ¿Ğ°Ğ´ĞµĞ½Ğ¸Ñ (Ğ°Ñ€Ñ‚Ğ¸ĞºÑƒĞ»Ñ‹, Ğ½Ğ¾Ğ¼ĞµÑ€Ğ°, Ğ˜ĞĞ)
    3. ĞĞ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµĞ¼ Ğ¸ Ğ´ĞµĞ´ÑƒĞ¿Ğ»Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµĞ¼
    """
    results = []
    seen_ids = set()
    
    # Ğ¡Ğ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº
    vector_results = search_emails_vector(query, limit=limit, time_context=time_context)
    for r in vector_results:
        source_id = r.get('source_id')
        if source_id and source_id in seen_ids:
            continue
        if source_id:
            seen_ids.add(source_id)
        results.append(r)
    
    # Ğ—Ğ°Ñ‚ĞµĞ¼ SQL Ğ¿Ğ¾Ğ¸ÑĞº Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… ÑĞ¾Ğ²Ğ¿Ğ°Ğ´ĞµĞ½Ğ¸Ğ¹
    sql_results = search_emails_sql(query, limit=limit)
    for r in sql_results:
        source_id = r.get('source_id')
        if source_id and source_id in seen_ids:
            continue
        if source_id:
            seen_ids.add(source_id)
        results.append(r)
    
    # Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€ÑƒĞµĞ¼ Ğ¿Ğ¾ final_score
    results.sort(key=lambda x: x.get('final_score', 0), reverse=True)
    
    logger.info(f"ĞŸĞ¾Ğ¸ÑĞº email: {len(results)} Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² (vector + sql)")
    return results[:limit]

def search_telegram_chats(query: str, limit: int = 30, time_context: dict = None) -> list:
    """
    ĞšĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ¿Ğ¾ Ñ‡Ğ°Ñ‚Ğ°Ğ¼:
    1. Ğ’ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº (ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹) â€” Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ğ¾ ÑĞ¼Ñ‹ÑĞ»Ñƒ Ñ ÑƒÑ‡Ñ‘Ñ‚Ğ¾Ğ¼ ÑĞ²ĞµĞ¶ĞµÑÑ‚Ğ¸
    2. SQL Ğ¿Ğ¾Ğ¸ÑĞº â€” Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¸Ñ‚ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ ÑĞ¾Ğ²Ğ¿Ğ°Ğ´ĞµĞ½Ğ¸Ñ
    3. ĞĞ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµĞ¼ Ğ¸ Ğ´ĞµĞ´ÑƒĞ¿Ğ»Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµĞ¼
    """
    results = []
    seen_content = set()
    
    # Ğ¡Ğ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº (Ñ ÑƒÑ‡Ñ‘Ñ‚Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°)
    vector_results = search_telegram_chats_vector(query, limit=limit, time_context=time_context)
    for r in vector_results:
        content_hash = hash(r['content'][:200])
        if content_hash not in seen_content:
            seen_content.add(content_hash)
            results.append(r)
    
    # Ğ—Ğ°Ñ‚ĞµĞ¼ SQL Ğ¿Ğ¾Ğ¸ÑĞº Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… ÑĞ¾Ğ²Ğ¿Ğ°Ğ´ĞµĞ½Ğ¸Ğ¹
    sql_results = search_telegram_chats_sql(query, limit=limit)
    for r in sql_results:
        content_hash = hash(r['content'][:200])
        if content_hash not in seen_content:
            seen_content.add(content_hash)
            results.append(r)
    
    # Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€ÑƒĞµĞ¼ Ğ¿Ğ¾ final_score (ĞµÑĞ»Ğ¸ ĞµÑÑ‚ÑŒ) Ğ¸Ğ»Ğ¸ similarity
    results.sort(key=lambda x: x.get('final_score', x.get('similarity', 0)), reverse=True)
    
    logger.info(f"ĞŸĞ¾Ğ¸ÑĞº Ğ² Ñ‡Ğ°Ñ‚Ğ°Ñ…: {len(results)} Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² (vector + sql)")
    return results[:limit]


def search_1c_data(query: str, limit: int = 30) -> list:
    """Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ¿Ğ¾ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼ 1Ğ¡ Ñ JOIN-Ğ°Ğ¼Ğ¸ Ğ¿Ğ¾ ÑĞ¿Ñ€Ğ°Ğ²Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ°Ğ¼.
    
    ĞŸÑ€Ğ¸Ğ¾Ñ€Ğ¸Ñ‚ĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ²:
    1. Ğ—Ğ°ĞºÑƒĞ¿Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ñ†ĞµĞ½Ñ‹ (purchase_prices)
    2. ĞŸÑ€Ğ¾Ğ´Ğ°Ğ¶Ğ¸ (sales) 
    3. Ğ—Ğ°ĞºĞ°Ğ·Ñ‹ ĞºĞ»Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ² (c1_customer_orders + items)
    4. Ğ—Ğ°ĞºĞ°Ğ·Ñ‹ Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ñ‰Ğ¸ĞºĞ°Ğ¼ (c1_supplier_orders + items)
    5. ĞŸÑ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ (c1_production + items)
    6. Ğ‘Ğ°Ğ½ĞºĞ¾Ğ²ÑĞºĞ¸Ğµ Ñ€Ğ°ÑÑ…Ğ¾Ğ´Ñ‹ (c1_bank_expenses)
    7. Ğ’Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½ĞµĞµ Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ĞµĞ½Ğ¸Ğµ (c1_internal_consumption + items)
    8. Ğ˜Ğ½Ğ²ĞµĞ½Ñ‚Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ (c1_inventory_count + items)
    9. ĞĞ¾Ğ¼ĞµĞ½ĞºĞ»Ğ°Ñ‚ÑƒÑ€Ğ° ÑĞ¿Ñ€Ğ°Ğ²Ğ¾Ñ‡Ğ½Ğ¸Ğº
    10. ĞšĞ»Ğ¸ĞµĞ½Ñ‚Ñ‹ ÑĞ¿Ñ€Ğ°Ğ²Ğ¾Ñ‡Ğ½Ğ¸Ğº
    """
    # ĞšĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¸ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ñ Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¸Ñ‚ĞµÑ‚Ğ°Ğ¼Ğ¸
    results_by_category = {
        "prices": [],        # Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¸Ñ‚ĞµÑ‚ 1
        "sales": [],         # Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¸Ñ‚ĞµÑ‚ 2
        "cust_orders": [],   # Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¸Ñ‚ĞµÑ‚ 3
        "supp_orders": [],   # Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¸Ñ‚ĞµÑ‚ 4
        "production": [],    # Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¸Ñ‚ĞµÑ‚ 5
        "bank": [],          # Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¸Ñ‚ĞµÑ‚ 6
        "consumption": [],   # Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¸Ñ‚ĞµÑ‚ 7
        "inventory": [],     # Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¸Ñ‚ĞµÑ‚ 8
        "nomenclature": [],  # Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¸Ñ‚ĞµÑ‚ 9
        "clients": [],       # Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¸Ñ‚ĞµÑ‚ 10
    }
    
    conn = get_db_connection()
    keywords = clean_keywords(query)
    
    if not keywords:
        return []
    
    # Ğ¡Ñ‚Ñ€Ğ¾Ğ¸Ğ¼ ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ğµ ILIKE Ğ´Ğ»Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ÑĞ»Ğ¾Ğ²
    def ilike_conditions(columns: list, keyword: str) -> tuple:
        """Ğ’Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ (SQL ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ğµ, Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹) Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¿Ğ¾ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼ ĞºĞ¾Ğ»Ğ¾Ğ½ĞºĞ°Ğ¼."""
        parts = []
        params = []
        for col in columns:
            parts.append(f"{col} ILIKE %s")
            params.append(f"%{keyword}%")
        return " OR ".join(parts), params
    
    try:
        with conn.cursor() as cur:
            for keyword in keywords[:3]:
                
                # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                # 1. Ğ—ĞĞšĞ£ĞŸĞĞ§ĞĞ«Ğ• Ğ¦Ğ•ĞĞ« (purchase_prices)
                # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                try:
                    cur.execute("""
                        SELECT doc_date, doc_number, contractor_name, 
                               nomenclature_name, quantity, price, sum_total 
                        FROM purchase_prices 
                        WHERE nomenclature_name ILIKE %s 
                           OR contractor_name ILIKE %s 
                        ORDER BY doc_date DESC LIMIT %s
                    """, (f"%{keyword}%", f"%{keyword}%", limit))
                    for row in cur.fetchall():
                        result = {
                            "source": "1Ğ¡: Ğ—ĞĞšĞ£ĞŸĞĞ§ĞĞ«Ğ• Ğ¦Ğ•ĞĞ«",
                            "date": row[0].strftime("%d.%m.%Y") if row[0] else "",
                            "content": f"{row[3]} Ğ¾Ñ‚ {row[2]}: {row[5]} Ñ€ÑƒĞ±./ĞµĞ´., "
                                       f"ĞºĞ¾Ğ»-Ğ²Ğ¾: {row[4]}, ÑÑƒĞ¼Ğ¼Ğ°: {row[6]} Ñ€ÑƒĞ±. (Ğ´Ğ¾Ğº. {row[1]})",
                            "type": "price"
                        }
                        if result not in results_by_category["prices"]:
                            results_by_category["prices"].append(result)
                except Exception as e:
                    logger.debug(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ·Ğ°ĞºÑƒĞ¿Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ñ†ĞµĞ½: {e}")
                
                # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                # 2. ĞŸĞ ĞĞ”ĞĞ–Ğ˜ (sales) â€” ÑƒĞ¶Ğµ Ğ´ĞµĞ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ°
                # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                try:
                    cur.execute("""
                        SELECT doc_date, doc_number, doc_type, client_name, 
                               nomenclature_name, quantity, price, sum_with_vat
                        FROM sales 
                        WHERE client_name ILIKE %s 
                           OR nomenclature_name ILIKE %s
                           OR consignee_name ILIKE %s
                        ORDER BY doc_date DESC LIMIT %s
                    """, (f"%{keyword}%", f"%{keyword}%", f"%{keyword}%", limit))
                    for row in cur.fetchall():
                        result = {
                            "source": f"1Ğ¡: ĞŸĞ ĞĞ”ĞĞ–Ğ˜ ({row[2]})",
                            "date": row[0].strftime("%d.%m.%Y") if row[0] else "",
                            "content": f"{row[4]} â†’ {row[3]}: {row[6]} Ñ€ÑƒĞ±./ĞµĞ´., "
                                       f"ĞºĞ¾Ğ»-Ğ²Ğ¾: {row[5]}, ÑÑƒĞ¼Ğ¼Ğ°: {row[7]} Ñ€ÑƒĞ±. (Ğ´Ğ¾Ğº. {row[1]})",
                            "type": "sales"
                        }
                        if result not in results_by_category["sales"]:
                            results_by_category["sales"].append(result)
                except Exception as e:
                    logger.debug(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¾Ğ´Ğ°Ğ¶: {e}")
                
                # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                # 3. Ğ—ĞĞšĞĞ—Ğ« ĞšĞ›Ğ˜Ğ•ĞĞ¢ĞĞ’ (c1_customer_orders + items)
                # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                try:
                    cur.execute("""
                        SELECT co.doc_date, co.doc_number, c.name as client,
                               n.name as product, coi.quantity, coi.price, coi.sum_total,
                               co.status, co.shipment_date
                        FROM c1_customer_orders co
                        JOIN c1_customer_order_items coi ON coi.order_key = co.ref_key
                        LEFT JOIN clients c ON co.partner_key = c.id::text
                        LEFT JOIN nomenclature n ON coi.nomenclature_key = n.id::text
                        WHERE (c.name ILIKE %s OR n.name ILIKE %s OR co.doc_number ILIKE %s)
                          AND co.is_deleted = false
                        ORDER BY co.doc_date DESC LIMIT %s
                    """, (f"%{keyword}%", f"%{keyword}%", f"%{keyword}%", limit))
                    for row in cur.fetchall():
                        shipment = f", Ğ¾Ñ‚Ğ³Ñ€ÑƒĞ·ĞºĞ°: {row[8].strftime('%d.%m.%Y')}" if row[8] else ""
                        result = {
                            "source": "1Ğ¡: Ğ—ĞĞšĞĞ—Ğ« ĞšĞ›Ğ˜Ğ•ĞĞ¢ĞĞ’",
                            "date": row[0].strftime("%d.%m.%Y") if row[0] else "",
                            "content": f"{row[3] or '?'} â†’ {row[2] or '?'}: {row[5]} Ñ€ÑƒĞ±., "
                                       f"ĞºĞ¾Ğ»-Ğ²Ğ¾: {row[4]}, ÑÑƒĞ¼Ğ¼Ğ°: {row[6]} Ñ€ÑƒĞ±. "
                                       f"(Ğ´Ğ¾Ğº. {row[1]}, ÑÑ‚Ğ°Ñ‚ÑƒÑ: {row[7] or '?'}{shipment})",
                            "type": "customer_order"
                        }
                        if result not in results_by_category["cust_orders"]:
                            results_by_category["cust_orders"].append(result)
                except Exception as e:
                    logger.debug(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ·Ğ°ĞºĞ°Ğ·Ğ¾Ğ² ĞºĞ»Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ²: {e}")
                
                # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                # 4. Ğ—ĞĞšĞĞ—Ğ« ĞŸĞĞ¡Ğ¢ĞĞ’Ğ©Ğ˜ĞšĞĞœ (c1_supplier_orders + items)
                # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                try:
                    cur.execute("""
                        SELECT so.doc_date, so.doc_number, c.name as supplier,
                               n.name as product, soi.quantity, soi.price, soi.sum_total,
                               so.status
                        FROM c1_supplier_orders so
                        JOIN c1_supplier_order_items soi ON soi.order_key = so.ref_key
                        LEFT JOIN clients c ON so.partner_key = c.id::text
                        LEFT JOIN nomenclature n ON soi.nomenclature_key = n.id::text
                        WHERE (c.name ILIKE %s OR n.name ILIKE %s OR so.doc_number ILIKE %s)
                          AND so.is_deleted = false
                        ORDER BY so.doc_date DESC LIMIT %s
                    """, (f"%{keyword}%", f"%{keyword}%", f"%{keyword}%", limit))
                    for row in cur.fetchall():
                        result = {
                            "source": "1Ğ¡: Ğ—ĞĞšĞĞ—Ğ« ĞŸĞĞ¡Ğ¢ĞĞ’Ğ©Ğ˜ĞšĞĞœ",
                            "date": row[0].strftime("%d.%m.%Y") if row[0] else "",
                            "content": f"{row[3] or '?'} Ğ¾Ñ‚ {row[2] or '?'}: {row[5]} Ñ€ÑƒĞ±., "
                                       f"ĞºĞ¾Ğ»-Ğ²Ğ¾: {row[4]}, ÑÑƒĞ¼Ğ¼Ğ°: {row[6]} Ñ€ÑƒĞ±. "
                                       f"(Ğ´Ğ¾Ğº. {row[1]}, ÑÑ‚Ğ°Ñ‚ÑƒÑ: {row[7] or '?'})",
                            "type": "supplier_order"
                        }
                        if result not in results_by_category["supp_orders"]:
                            results_by_category["supp_orders"].append(result)
                except Exception as e:
                    logger.debug(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ·Ğ°ĞºĞ°Ğ·Ğ¾Ğ² Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ñ‰Ğ¸ĞºĞ°Ğ¼: {e}")
                
                # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                # 5. ĞŸĞ ĞĞ˜Ğ—Ğ’ĞĞ”Ğ¡Ğ¢Ğ’Ğ (c1_production + items)
                # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                try:
                    cur.execute("""
                        SELECT p.doc_date, p.doc_number, 
                               n.name as product, pi.quantity, pi.price, pi.sum_total
                        FROM c1_production p
                        JOIN c1_production_items pi ON pi.production_key = p.ref_key
                        LEFT JOIN nomenclature n ON pi.nomenclature_key = n.id::text
                        WHERE (n.name ILIKE %s OR p.doc_number ILIKE %s OR p.comment ILIKE %s)
                          AND p.is_deleted = false
                        ORDER BY p.doc_date DESC LIMIT %s
                    """, (f"%{keyword}%", f"%{keyword}%", f"%{keyword}%", limit))
                    for row in cur.fetchall():
                        result = {
                            "source": "1Ğ¡: ĞŸĞ ĞĞ˜Ğ—Ğ’ĞĞ”Ğ¡Ğ¢Ğ’Ğ",
                            "date": row[0].strftime("%d.%m.%Y") if row[0] else "",
                            "content": f"{row[2] or '?'}: ĞºĞ¾Ğ»-Ğ²Ğ¾: {row[3]}, "
                                       f"Ñ†ĞµĞ½Ğ°: {row[4]} Ñ€ÑƒĞ±., ÑÑƒĞ¼Ğ¼Ğ°: {row[5]} Ñ€ÑƒĞ±. (Ğ´Ğ¾Ğº. {row[1]})",
                            "type": "production"
                        }
                        if result not in results_by_category["production"]:
                            results_by_category["production"].append(result)
                except Exception as e:
                    logger.debug(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ°: {e}")
                
                # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                # 6. Ğ‘ĞĞĞšĞĞ’Ğ¡ĞšĞ˜Ğ• Ğ ĞĞ¡Ğ¥ĞĞ”Ğ« (c1_bank_expenses)
                # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                try:
                    cur.execute("""
                        SELECT be.doc_date, be.doc_number, c.name as counterparty,
                               be.amount, be.purpose, be.comment
                        FROM c1_bank_expenses be
                        LEFT JOIN clients c ON be.counterparty_key = c.id::text
                        WHERE (c.name ILIKE %s OR be.purpose ILIKE %s 
                               OR be.comment ILIKE %s OR be.doc_number ILIKE %s)
                          AND be.is_deleted = false
                        ORDER BY be.doc_date DESC LIMIT %s
                    """, (f"%{keyword}%", f"%{keyword}%", f"%{keyword}%", f"%{keyword}%", limit))
                    for row in cur.fetchall():
                        purpose = row[4][:100] if row[4] else ""
                        result = {
                            "source": "1Ğ¡: Ğ‘ĞĞĞšĞĞ’Ğ¡ĞšĞ˜Ğ• Ğ ĞĞ¡Ğ¥ĞĞ”Ğ«",
                            "date": row[0].strftime("%d.%m.%Y") if row[0] else "",
                            "content": f"{row[2] or '?'}: {row[3]} Ñ€ÑƒĞ±. "
                                       f"ĞĞ°Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ: {purpose} (Ğ´Ğ¾Ğº. {row[1]})",
                            "type": "bank_expense"
                        }
                        if result not in results_by_category["bank"]:
                            results_by_category["bank"].append(result)
                except Exception as e:
                    logger.debug(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ±Ğ°Ğ½ĞºĞ¾Ğ²ÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑ…Ğ¾Ğ´Ğ¾Ğ²: {e}")
                
                # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                # 7. Ğ’ĞĞ£Ğ¢Ğ Ğ•ĞĞĞ•Ğ• ĞŸĞĞ¢Ğ Ğ•Ğ‘Ğ›Ğ•ĞĞ˜Ğ• (c1_internal_consumption + items)
                # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                try:
                    cur.execute("""
                        SELECT ic.doc_date, ic.doc_number,
                               n.name as product, ici.quantity, ici.sum_total
                        FROM c1_internal_consumption ic
                        JOIN c1_internal_consumption_items ici ON ici.doc_key = ic.ref_key
                        LEFT JOIN nomenclature n ON ici.nomenclature_key = n.id::text
                        WHERE (n.name ILIKE %s OR ic.doc_number ILIKE %s OR ic.comment ILIKE %s)
                          AND ic.is_deleted = false
                        ORDER BY ic.doc_date DESC LIMIT %s
                    """, (f"%{keyword}%", f"%{keyword}%", f"%{keyword}%", limit))
                    for row in cur.fetchall():
                        result = {
                            "source": "1Ğ¡: Ğ’ĞĞ£Ğ¢Ğ Ğ•ĞĞĞ•Ğ• ĞŸĞĞ¢Ğ Ğ•Ğ‘Ğ›Ğ•ĞĞ˜Ğ•",
                            "date": row[0].strftime("%d.%m.%Y") if row[0] else "",
                            "content": f"{row[2] or '?'}: ĞºĞ¾Ğ»-Ğ²Ğ¾: {row[3]}, "
                                       f"ÑÑƒĞ¼Ğ¼Ğ°: {row[4]} Ñ€ÑƒĞ±. (Ğ´Ğ¾Ğº. {row[1]})",
                            "type": "consumption"
                        }
                        if result not in results_by_category["consumption"]:
                            results_by_category["consumption"].append(result)
                except Exception as e:
                    logger.debug(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½ĞµĞ³Ğ¾ Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ĞµĞ½Ğ¸Ñ: {e}")
                
                # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                # 8. Ğ˜ĞĞ’Ğ•ĞĞ¢ĞĞ Ğ˜Ğ—ĞĞ¦Ğ˜Ğ¯ (c1_inventory_count + items)
                # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                try:
                    cur.execute("""
                        SELECT inv.doc_date, inv.doc_number,
                               n.name as product, ii.quantity_fact, 
                               ii.quantity_account, ii.deviation
                        FROM c1_inventory_count inv
                        JOIN c1_inventory_count_items ii ON ii.doc_key = inv.ref_key
                        LEFT JOIN nomenclature n ON ii.nomenclature_key = n.id::text
                        WHERE (n.name ILIKE %s OR inv.doc_number ILIKE %s)
                          AND inv.is_deleted = false
                        ORDER BY inv.doc_date DESC LIMIT %s
                    """, (f"%{keyword}%", f"%{keyword}%", limit))
                    for row in cur.fetchall():
                        deviation = row[5] if row[5] else 0
                        dev_str = f"+{deviation}" if deviation > 0 else str(deviation)
                        result = {
                            "source": "1Ğ¡: Ğ˜ĞĞ’Ğ•ĞĞ¢ĞĞ Ğ˜Ğ—ĞĞ¦Ğ˜Ğ¯",
                            "date": row[0].strftime("%d.%m.%Y") if row[0] else "",
                            "content": f"{row[2] or '?'}: Ñ„Ğ°ĞºÑ‚: {row[3]}, ÑƒÑ‡Ñ‘Ñ‚: {row[4]}, "
                                       f"Ğ¾Ñ‚ĞºĞ»Ğ¾Ğ½ĞµĞ½Ğ¸Ğµ: {dev_str} (Ğ´Ğ¾Ğº. {row[1]})",
                            "type": "inventory"
                        }
                        if result not in results_by_category["inventory"]:
                            results_by_category["inventory"].append(result)
                except Exception as e:
                    logger.debug(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¸Ğ½Ğ²ĞµĞ½Ñ‚Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸: {e}")
                
                # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                # 9. ĞĞĞœĞ•ĞĞšĞ›ĞĞ¢Ğ£Ğ Ğ (ÑĞ¿Ñ€Ğ°Ğ²Ğ¾Ñ‡Ğ½Ğ¸Ğº)
                # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                try:
                    cur.execute("""
                        SELECT name, code, unit FROM nomenclature 
                        WHERE name ILIKE %s OR code ILIKE %s 
                        LIMIT %s
                    """, (f"%{keyword}%", f"%{keyword}%", limit))
                    for row in cur.fetchall():
                        result = {
                            "source": "1Ğ¡: ĞĞ¾Ğ¼ĞµĞ½ĞºĞ»Ğ°Ñ‚ÑƒÑ€Ğ°",
                            "content": f"{row[0]} (ĞºĞ¾Ğ´: {row[1]}, ĞµĞ´.: {row[2]})",
                            "type": "nomenclature"
                        }
                        if result not in results_by_category["nomenclature"]:
                            results_by_category["nomenclature"].append(result)
                except Exception as e:
                    logger.debug(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ½Ğ¾Ğ¼ĞµĞ½ĞºĞ»Ğ°Ñ‚ÑƒÑ€Ñ‹: {e}")
                
                # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                # 10. ĞšĞ›Ğ˜Ğ•ĞĞ¢Ğ« (ÑĞ¿Ñ€Ğ°Ğ²Ğ¾Ñ‡Ğ½Ğ¸Ğº)
                # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                try:
                    cur.execute("""
                        SELECT name, inn FROM clients 
                        WHERE name ILIKE %s OR inn ILIKE %s 
                        LIMIT %s
                    """, (f"%{keyword}%", f"%{keyword}%", limit))
                    for row in cur.fetchall():
                        result = {
                            "source": "1Ğ¡: ĞšĞ»Ğ¸ĞµĞ½Ñ‚Ñ‹",
                            "content": f"{row[0]} (Ğ˜ĞĞ: {row[1]})",
                            "type": "client"
                        }
                        if result not in results_by_category["clients"]:
                            results_by_category["clients"].append(result)
                except Exception as e:
                    logger.debug(f"ĞÑˆĞ¸Ğ±ĞºĞ° ĞºĞ»Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ²: {e}")
    
    finally:
        conn.close()
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Ğ¡Ğ‘ĞĞ ĞšĞ Ğ Ğ•Ğ—Ğ£Ğ›Ğ¬Ğ¢ĞĞ¢ĞĞ’ ĞŸĞ ĞŸĞ Ğ˜ĞĞ Ğ˜Ğ¢Ğ•Ğ¢Ğ£
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ĞŸĞ¾Ñ€ÑĞ´Ğ¾Ğº ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¹ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¸Ñ‚ĞµÑ‚
    category_order = [
        "prices", "sales", "cust_orders", "supp_orders",
        "production", "bank", "consumption", "inventory",
        "nomenclature", "clients"
    ]
    
    final_results = []
    for cat in category_order:
        items = results_by_category[cat]
        remaining = limit - len(final_results)
        if remaining <= 0:
            break
        final_results.extend(items[:remaining])
    
    # Ğ›Ğ¾Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ
    counts = {cat: len(items) for cat, items in results_by_category.items() if items}
    logger.info(f"ĞŸĞ¾Ğ¸ÑĞº 1Ğ¡ Ğ¿Ğ¾ {keywords}: {counts}, Ğ¸Ñ‚Ğ¾Ğ³Ğ¾: {len(final_results)}")
    
    return final_results[:limit]


def search_internet(query: str) -> tuple:
    """ĞŸĞ¾Ğ¸ÑĞº Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€Ğ½ĞµÑ‚Ğµ Ñ‡ĞµÑ€ĞµĞ· Perplexity. Ğ’Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ (Ñ‚ĞµĞºÑÑ‚, ÑĞ¿Ğ¸ÑĞ¾Ğº_ÑÑÑ‹Ğ»Ğ¾Ğº)."""
    if not ROUTERAI_API_KEY:
        return "", []
    try:
        response = requests.post(
            f"{ROUTERAI_BASE_URL}/chat/completions",
            headers={"Authorization": f"Bearer {ROUTERAI_API_KEY}", "Content-Type": "application/json"},
            json={"model": "perplexity/sonar", "messages": [{"role": "user", "content": query}]},
            timeout=60
        )
        result = response.json()
        
        if "choices" not in result:
            return "", []
        
        text = result["choices"][0]["message"]["content"]
        citations = result.get("citations", [])
        
        return text, citations
        
    except Exception as e:
        logger.error(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¸Ğ½Ñ‚ĞµÑ€Ğ½ĞµÑ‚: {e}")
        return "", []


def generate_response(question: str, db_results: list, web_results: str, web_citations: list = None, chat_context: str = "") -> str:
    """Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."""
    if not ROUTERAI_API_KEY:
        return "API ĞºĞ»ÑÑ‡ Ğ½Ğµ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾ĞµĞ½"
    try:
        context_parts = []
        
        # Ğ“Ñ€ÑƒĞ¿Ğ¿Ğ¸Ñ€ÑƒĞµĞ¼ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ Ñ‚Ğ¸Ğ¿Ñƒ
        prices = [r for r in db_results if r.get('type') == 'price']
        other_1c = [r for r in db_results if r.get('source', '').startswith('1Ğ¡') and r.get('type') != 'price']
        chats = [r for r in db_results if r.get('source', '').startswith('Ğ§Ğ°Ñ‚')]
        emails = [r for r in db_results if r.get('source', '').startswith('Email')]
        
        # Ğ¡Ğ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ·Ğ°ĞºÑƒĞ¿Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ñ†ĞµĞ½Ñ‹ (ĞŸĞ Ğ˜ĞĞ Ğ˜Ğ¢Ğ•Ğ¢!)
        if prices:
            context_parts.append("=== Ğ—ĞĞšĞ£ĞŸĞĞ§ĞĞ«Ğ• Ğ¦Ğ•ĞĞ« ĞšĞĞœĞŸĞĞĞ˜Ğ˜ (Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ 1Ğ¡) ===")
            for i, res in enumerate(prices, 1):
                context_parts.append(f"{i}. {res.get('date', '')} {res['content']}")
        
        # ĞŸĞ¾Ñ‚Ğ¾Ğ¼ ÑĞ¿Ñ€Ğ°Ğ²Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¸ 1Ğ¡
        if other_1c:
            context_parts.append("\n=== Ğ¡ĞŸĞ ĞĞ’ĞĞ§ĞĞ˜ĞšĞ˜ 1Ğ¡ ===")
            for i, res in enumerate(other_1c, 1):
                context_parts.append(f"{i}. [{res['source']}] {res['content'][:300]}")
        
        # ĞŸĞ¾Ñ‚Ğ¾Ğ¼ Ñ‡Ğ°Ñ‚Ñ‹
        if chats:
            context_parts.append("\n=== Ğ˜Ğ— Ğ§ĞĞ¢ĞĞ’ ===")
            for i, res in enumerate(chats[:5], 1):
                score_info = ""
                if 'final_score' in res:
                    score_info = f" [Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ: {res['final_score']:.0%}]"
                elif 'similarity' in res:
                    score_info = f" [Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ: {res['similarity']:.0%}]"
                date_info = f" ({res['date']})" if res.get('date') else ""
                context_parts.append(f"{i}.{score_info}{date_info} {res['content'][:300]}")
        
        # ĞŸĞ¾Ñ‚Ğ¾Ğ¼ email
        if emails:
            context_parts.append("\n=== Ğ˜Ğ— EMAIL ===")
            for i, res in enumerate(emails[:5], 1):
                score_info = f" [Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ: {res.get('final_score', res.get('similarity', 0)):.0%}]"
                date_info = f" ({res['date']})" if res.get('date') else ""
                subj = (res.get("subject") or "").strip()
                frm = (res.get("from_address") or "").strip()
                header = ""
                if subj or frm:
                    header = f"{subj} | {frm}".strip(" |")

                context_parts.append(
                    f"{i}.{score_info}{date_info} {header}\n{res['content'][:400]}"
                )

        
        # Ğ˜Ğ½Ñ‚ĞµÑ€Ğ½ĞµÑ‚
        if web_results:
            context_parts.append("\n=== Ğ˜ĞĞ¢Ğ•Ğ ĞĞ•Ğ¢ ===")
            context_parts.append(web_results[:2000])
        
        context = "\n".join(context_parts)
        
        company_profile = get_company_profile()
        
        prompt = f"""{company_profile}

Ğ¢Ñ‹ â€” RAG-Ğ°Ğ³ĞµĞ½Ñ‚ ĞºĞ¾Ğ¼Ğ¿Ğ°Ğ½Ğ¸Ğ¸ Ğ¤Ñ€ÑƒĞ¼ĞµĞ»Ğ°Ğ´. ĞÑ‚Ğ²ĞµÑ‡Ğ°Ğ¹ Ğ½Ğ° Ñ€ÑƒÑÑĞºĞ¾Ğ¼.

Ğ’ĞĞŸĞ ĞĞ¡: {question}

ĞĞĞ™Ğ”Ğ•ĞĞĞ«Ğ• Ğ”ĞĞĞĞ«Ğ•:
{context if context else "ĞĞ¸Ñ‡ĞµĞ³Ğ¾ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾."}

Ğ˜ĞĞ¡Ğ¢Ğ Ğ£ĞšĞ¦Ğ˜Ğ˜:
1. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞ¹ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ· Ğ¿Ñ€Ğ¾Ñ„Ğ¸Ğ»Ñ ĞºĞ¾Ğ¼Ğ¿Ğ°Ğ½Ğ¸Ğ¸ Ğ¸ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°
2. Ğ”Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ· 1Ğ¡ (Ğ·Ğ°ĞºÑƒĞ¿ĞºĞ¸, Ğ¿Ñ€Ğ¾Ğ´Ğ°Ğ¶Ğ¸, Ğ½Ğ¾Ğ¼ĞµĞ½ĞºĞ»Ğ°Ñ‚ÑƒÑ€Ğ°) â€” ÑÑ‚Ğ¾ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ°Ğ½Ğ¸Ğ¸
3. Ğ”Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ· Ğ§ĞĞ¢ĞĞ’ Ğ¸ EMAIL â€” Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½ÑÑ Ğ¿ĞµÑ€ĞµĞ¿Ğ¸ÑĞºĞ° ÑĞ¾Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¸ĞºĞ¾Ğ²
4. Ğ£ĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ğ¹ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğµ Ñ†Ğ¸Ñ„Ñ€Ñ‹, Ğ´Ğ°Ñ‚Ñ‹, Ğ¸Ğ¼ĞµĞ½Ğ° â€” ĞµÑĞ»Ğ¸ Ğ¾Ğ½Ğ¸ ĞµÑÑ‚ÑŒ Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
5. Ğ•ÑĞ»Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ â€” ÑĞºĞ°Ğ¶Ğ¸ Ğ¾Ğ± ÑÑ‚Ğ¾Ğ¼, Ğ½Ğµ Ğ¿Ñ€Ğ¸Ğ´ÑƒĞ¼Ñ‹Ğ²Ğ°Ğ¹
6. ĞÑ‚Ğ²ĞµÑ‡Ğ°Ğ¹ Ğ¿Ğ¾ ÑÑƒÑ‰ĞµÑÑ‚Ğ²Ñƒ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ°, ĞºÑ€Ğ°Ñ‚ĞºĞ¾ Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾

ĞÑ‚Ğ²ĞµÑ‚:"""

        response = requests.post(f"{ROUTERAI_BASE_URL}/chat/completions", headers={"Authorization": f"Bearer {ROUTERAI_API_KEY}", "Content-Type": "application/json"}, json={"model": "google/gemini-3-flash-preview", "messages": [{"role": "user", "content": prompt}], "max_tokens": 2000}, timeout=60)
        result = response.json()
        if "choices" in result:
            response_text = result["choices"][0]["message"]["content"]
            
            if web_citations:
                response_text += "\n\nğŸ“ **Ğ˜ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¸:**"
                for i, url in enumerate(web_citations[:5], 1):
                    response_text += f"\n{i}. {url}"
            
            return response_text
        return "ĞÑˆĞ¸Ğ±ĞºĞ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸"
    except Exception as e:
        return f"ĞÑˆĞ¸Ğ±ĞºĞ°: {e}"


def classify_question(question: str) -> dict:
    """ĞšĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ° Ğ´Ğ»Ñ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ² Ğ¿Ğ¾Ğ¸ÑĞºĞ°."""
    if not ROUTERAI_API_KEY:
        return {"search_1c": True, "search_chats": True, "search_email": True, "search_web": False, "keywords": question, "priority": "1c"}
    try:
        prompt = f"""ĞĞ¿Ñ€ĞµĞ´ĞµĞ»Ğ¸ Ğ³Ğ´Ğµ Ğ¸ÑĞºĞ°Ñ‚ÑŒ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ.
Ğ˜ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¸: 1Ğ¡ (Ñ†ĞµĞ½Ñ‹, Ğ·Ğ°ĞºÑƒĞ¿ĞºĞ¸, Ñ‚Ğ¾Ğ²Ğ°Ñ€Ñ‹), Ğ§Ğ°Ñ‚Ñ‹ (Ğ¾Ğ±ÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Telegram), Email (Ğ¿ĞµÑ€ĞµĞ¿Ğ¸ÑĞºĞ° Ğ¿Ğ¾ Ğ¿Ğ¾Ñ‡Ñ‚Ğµ), Ğ˜Ğ½Ñ‚ĞµÑ€Ğ½ĞµÑ‚ (Ğ²Ğ½ĞµÑˆĞ½ÑÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ).
Ğ˜Ğ·Ğ²Ğ»ĞµĞºĞ¸ 1-3 ĞšĞ›Ğ®Ğ§Ğ•Ğ’Ğ«Ğ¥ Ğ¡Ğ›ĞĞ’Ğ (ÑÑƒÑ‰ĞµÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ±ĞµĞ· Ğ·Ğ°Ğ¿ÑÑ‚Ñ‹Ñ…: ÑĞ°Ñ…Ğ°Ñ€ Ğ¼ÑƒĞºĞ° Ñ‚Ğ¾Ñ€Ñ‚)

Ğ’Ğ¾Ğ¿Ñ€Ğ¾Ñ: {question}

JSON: {{"search_1c": true/false, "search_chats": true/false, "search_email": true/false, "search_web": true/false, "keywords": "ÑĞ»Ğ¾Ğ²Ğ¾1 ÑĞ»Ğ¾Ğ²Ğ¾2", "priority": "1c/chats/email/web"}}"""
        response = requests.post(f"{ROUTERAI_BASE_URL}/chat/completions", headers={"Authorization": f"Bearer {ROUTERAI_API_KEY}", "Content-Type": "application/json"}, json={"model": "google/gemini-3-flash-preview", "messages": [{"role": "user", "content": prompt}], "max_tokens": 200}, timeout=30)
        result = response.json()
        if "choices" in result:
            match = re.search(r'\{[^}]+\}', result["choices"][0]["message"]["content"])
            if match:
                return json.loads(match.group())
        return {"search_1c": True, "search_chats": True, "search_email": True, "search_web": False, "keywords": question, "priority": "1c"}
    except:
        return {"search_1c": True, "search_chats": True, "search_email": True, "search_web": False, "keywords": question, "priority": "1c"}

def rerank_results(question: str, results: list, top_k: int = 10) -> list:
    """
    ĞŸĞµÑ€ĞµÑ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· LLM.
    Ğ‘ĞµÑ€Ñ‘Ñ‚ Ğ´Ğ¾ 60 ĞºĞ°Ğ½Ğ´Ğ¸Ğ´Ğ°Ñ‚Ğ¾Ğ², Ğ¿Ñ€Ğ¾ÑĞ¸Ñ‚ GPT Ğ¾Ñ†ĞµĞ½Ğ¸Ñ‚ÑŒ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ²Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ top_k Ğ»ÑƒÑ‡ÑˆĞ¸Ñ….
    """
    if not results or not ROUTERAI_API_KEY:
        return results[:top_k]
    
    # Ğ‘ĞµÑ€Ñ‘Ğ¼ Ğ¼Ğ°ĞºÑĞ¸Ğ¼ÑƒĞ¼ 60 ĞºĞ°Ğ½Ğ´Ğ¸Ğ´Ğ°Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ reranking
    candidates = results[:60]
    
    if len(candidates) <= top_k:
        return candidates
    
    # Ğ¤Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµĞ¼ ÑĞ¿Ğ¸ÑĞ¾Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸
    docs_text = []
    for i, r in enumerate(candidates):
        source = r.get('source', 'Unknown')
        content = r.get('content', '')[:300]
        date = r.get('date', '')
        docs_text.append(f"[{i}] ({source}, {date}) {content}")
    
    docs_joined = "\n".join(docs_text)
    
    prompt = f"""ĞÑ†ĞµĞ½Ğ¸ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ°.

Ğ’ĞĞŸĞ ĞĞ¡: {question}

Ğ”ĞĞšĞ£ĞœĞ•ĞĞ¢Ğ«:
{docs_joined}

Ğ’ĞµÑ€Ğ½Ğ¸ Ğ¢ĞĞ›Ğ¬ĞšĞ Ğ½Ğ¾Ğ¼ĞµÑ€Ğ° {top_k} ÑĞ°Ğ¼Ñ‹Ñ… Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ·Ğ°Ğ¿ÑÑ‚ÑƒÑ, Ğ¾Ñ‚ Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ Ğº Ñ…ÑƒĞ´ÑˆĞµĞ¼Ñƒ.
ĞŸÑ€Ğ¸Ğ¼ĞµÑ€ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°: 3,7,1,4,9,2,0,5,8,6

ĞĞ¾Ğ¼ĞµÑ€Ğ°:"""

    try:
        response = requests.post(
            f"{ROUTERAI_BASE_URL}/chat/completions",
            headers={
                "Authorization": f"Bearer {ROUTERAI_API_KEY}",
                "Content-Type": "application/json"
            },
            json={
                "model": "openai/gpt-4.1-mini",
                "messages": [{"role": "user", "content": prompt}],
                "max_tokens": 2000,
                "temperature": 0
            },
            timeout=30
        )
        
        result = response.json()
        if "choices" not in result:
            logger.warning(f"Rerank: Ğ½ĞµÑ‚ choices Ğ² Ğ¾Ñ‚Ğ²ĞµÑ‚Ğµ")
            return candidates[:top_k]
        
        answer = result["choices"][0]["message"]["content"].strip()
        
        # ĞŸĞ°Ñ€ÑĞ¸Ğ¼ Ğ½Ğ¾Ğ¼ĞµÑ€Ğ°
        indices = []
        for part in answer.replace(" ", "").split(","):
            try:
                idx = int(part.strip())
                if 0 <= idx < len(candidates) and idx not in indices:
                    indices.append(idx)
            except ValueError:
                continue
        
        if not indices:
            logger.warning(f"Rerank: Ğ½Ğµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ Ñ€Ğ°ÑĞ¿Ğ°Ñ€ÑĞ¸Ñ‚ÑŒ Ğ¾Ñ‚Ğ²ĞµÑ‚ '{answer}'")
            return candidates[:top_k]
        
        # Ğ¡Ğ¾Ğ±Ğ¸Ñ€Ğ°ĞµĞ¼ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ğ½Ğ¾Ğ²Ğ¾Ğ¼ Ğ¿Ğ¾Ñ€ÑĞ´ĞºĞµ
        reranked = [candidates[i] for i in indices[:top_k]]
        
        # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ¾ÑÑ‚Ğ°Ğ²ÑˆĞ¸ĞµÑÑ ĞµÑĞ»Ğ¸ Ğ½Ğµ Ñ…Ğ²Ğ°Ñ‚Ğ°ĞµÑ‚
        if len(reranked) < top_k:
            for r in candidates:
                if r not in reranked:
                    reranked.append(r)
                if len(reranked) >= top_k:
                    break
        
        logger.info(f"Rerank: {len(candidates)} -> {len(reranked)} (top {top_k})")
        return reranked
        
    except Exception as e:
        logger.error(f"ĞÑˆĞ¸Ğ±ĞºĞ° reranking: {e}")
        return candidates[:top_k]

async def process_rag_query(question: str, chat_context: str = "") -> str:
    """ĞÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ RAG-Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ° Ñ ÑƒÑ‡Ñ‘Ñ‚Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°."""
    logger.info(f"RAG Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ: {question}")
    
    # Ğ˜Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµĞ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ¸Ğ· Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ°
    time_context = extract_time_context(question)
    if time_context["has_time_filter"]:
        logger.info(f"Ğ’Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚: decay_days={time_context['decay_days']}, fw={time_context['freshness_weight']}")
    
    # ĞšĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµĞ¼ Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ
    classification = classify_question(question)
    logger.info(f"ĞšĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ: {classification}")
    
    keywords = classification.get("keywords", question)
    db_results = []
    
    # ĞŸĞ¾Ğ¸ÑĞº Ğ² 1Ğ¡ (SQL) â€” Ğ²ÑĞµĞ³Ğ´Ğ° Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¼
    if classification.get("search_1c", True):
        c1_results = search_1c_data(keywords, limit=30)
        db_results.extend(c1_results)
        logger.info(f"ĞĞ°Ğ¹Ğ´ĞµĞ½Ğ¾ Ğ² 1Ğ¡: {len(c1_results)}")
    
    # ĞŸĞ¾Ğ¸ÑĞº Ğ² Ñ‡Ğ°Ñ‚Ğ°Ñ… (Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ñ‹Ğ¹ Ñ ÑƒÑ‡Ñ‘Ñ‚Ğ¾Ğ¼ ÑĞ²ĞµĞ¶ĞµÑÑ‚Ğ¸ + SQL)
    if classification.get("search_chats", True):
        chat_results = search_telegram_chats(keywords, limit=30, time_context=time_context)
        db_results.extend(chat_results)
        logger.info(f"ĞĞ°Ğ¹Ğ´ĞµĞ½Ğ¾ Ğ² Ñ‡Ğ°Ñ‚Ğ°Ñ…: {len(chat_results)}")
    
    # ĞŸĞ¾Ğ¸ÑĞº Ğ² email (Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ñ‹Ğ¹ Ñ ÑƒÑ‡Ñ‘Ñ‚Ğ¾Ğ¼ ÑĞ²ĞµĞ¶ĞµÑÑ‚Ğ¸)
    if classification.get("search_email", True):
        email_results = search_emails(keywords, limit=30, time_context=time_context)
        db_results.extend(email_results)
        logger.info(f"ĞĞ°Ğ¹Ğ´ĞµĞ½Ğ¾ Ğ² email: {len(email_results)}")
    
    logger.info(f"Ğ’ÑĞµĞ³Ğ¾ Ğ² Ğ‘Ğ”: {len(db_results)}")
    
    # Reranking â€” Ğ¿ĞµÑ€ĞµÑ€Ğ°Ğ½Ğ¶Ğ¸Ñ€ÑƒĞµĞ¼ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ñ‡ĞµÑ€ĞµĞ· LLM
    if len(db_results) > 10:
        db_results = rerank_results(question, db_results, top_k=15)
    
    # ĞŸĞ¾Ğ¸ÑĞº Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€Ğ½ĞµÑ‚Ğµ
    web_results = ""
    web_citations = []
    if classification.get("search_web", False):
        web_results, web_citations = search_internet(question)
    
    return generate_response(question, db_results, web_results, web_citations, chat_context)

async def index_new_message(table_name: str, message_id: int, content: str):
    """Ğ˜Ğ½Ğ´ĞµĞºÑĞ¸Ñ€ÑƒĞµÑ‚ Ğ½Ğ¾Ğ²Ğ¾Ğµ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ°."""
    if not VECTOR_SEARCH_ENABLED:
        return
    
    if not content or len(content.strip()) < 10:
        return
    
    try:
        index_telegram_message(table_name, message_id, content)
        logger.debug(f"ĞŸÑ€Ğ¾Ğ¸Ğ½Ğ´ĞµĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¾ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ {message_id} Ğ¸Ğ· {table_name}")
    except Exception as e:
        logger.error(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¸Ğ½Ğ´ĞµĞºÑĞ°Ñ†Ğ¸Ğ¸ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ: {e}")
